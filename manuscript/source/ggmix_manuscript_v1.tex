\documentclass[12pt,letter]{article}\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}

%########################################################################################
%            						PACKAGES
%########################################################################################

\usepackage{authblk} % for author affiliations
\usepackage{float} % for H in figures and tables
\usepackage{amsmath,amsthm,amssymb,bbm,mathrsfs,mathtools,xfrac} %math stuff

\usepackage[sort, numbers]{natbib}   % bibliography omit 'round' option if you prefer square brackets
\usepackage{placeins} % for \FloatBarrier
\usepackage[pagebackref=true,bookmarks]{hyperref}
\hypersetup{
	unicode=false,
	pdftoolbar=true,
	pdfmenubar=true,
	pdffitwindow=false,     % window fit to page when opened
	pdfstartview={FitH},    % fits the width of the page to the window
	pdftitle={Penalized LMM in Families},    % title
	pdfauthor={Sahir Rai Bhatnagar},     % author
	pdfsubject={Subject},   % subject of the document
	pdfcreator={Sahir Rai Bhatnagar},   % creator of the document
	pdfproducer={Sahir Rai Bhatnagar}, % producer of the document
	pdfkeywords={}, % list of keywords
	pdfnewwindow=true,      % links in new window
	colorlinks=true,       % false: boxed links; true: colored links
	linkcolor=red,          % color of internal links (change box color with linkbordercolor)
	citecolor=blue,        % color of links to bibliography
	filecolor=black,      % color of file links
	urlcolor=cyan           % color of external links
}
\usepackage[utf8]{inputenc} % for french accents
\usepackage[T1]{fontenc} % for french accents
\usepackage{ctable} % load after tikz. used for tables
\usepackage{array}
\newcolumntype{L}{>{\centering\arraybackslash}m{3cm}} % used for text wrapping in ctable
\usepackage{color, colortbl, xcolor, comment}
\usepackage{subfig}
\usepackage{tabu}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{tcolorbox} % for box around text
%\usepackage[ruled,vlined,linesnumbered,noresetcount]{algorithm2e}
\usepackage[ruled,vlined,noresetcount]{algorithm2e}
%\usepackage[american]{babel}
%\let\tnote\relax
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}{Proposition}[section]

%\usepackage{csquotes}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{tabularx,ragged2e,booktabs,caption}
\usepackage{graphicx}

\usepackage{lineno}
\linenumbers
%\usepackage[style=apa,sortcites=true,sorting=nyt,backend=biber]{biblatex}
%\usepackage{epstopdf}
\usepackage{chngcntr} % for resetting figure number in appendix

%\usepackage{tabulary}
\usepackage{siunitx}
%\sisetup{output-exponent-marker=\ensuremath{\mathrm{e}}}
%\usepackage{setspace}
%\AtBeginEnvironment{tabulary}{\onehalfspacing}
%\usepackage{multirow}
%\usepackage{ctable} % NEED TO LOAD CTABLE AFTER TIKZ FOR SOME REASON
%\usepackage{array}
%\newcolumntype{L}{>{\centering\arraybackslash}m{3cm}} % used for text wrapping in ctable
%\usepackage{enumitem}
% These packages are all incorporated in the memoir class to one degree or another...

%########################################################################################
%            						CUSTOM COMMANDS
%########################################################################################

\newcommand{\tm}[1]{\textrm{{#1}}}
\newcommand{\bA}{\mb{A}}
\newcommand{\bB}{\mb{B}}
\newcommand{\bC}{\mb{C}}

%\usepackage{endfloat} 

\newcommand{\bx}{\textbf{\emph{x}}}
\newcommand{\by}{\textbf{\emph{y}}}
\newcommand{\bX}{\textbf{X}}
\newcommand{\bW}{\textbf{W}}
\newcommand{\bY}{\textbf{Y}}
\newcommand{\bD}{\textbf{D}}
\newcommand{\bH}{\textbf{H}}
\newcommand{\ggmix}{\texttt{ggmix}}
\newcommand{\trans}{\top}
\newcommand{\bXtilde}{\widetilde{\bX}}
\newcommand{\bYtilde}{\widetilde{\bY}}
\newcommand{\bDtilde}{\widetilde{\bD}}
\newcommand{\Xtilde}{\widetilde{X}}
\newcommand{\Ytilde}{\widetilde{Y}}
\newcommand{\Dtilde}{\widetilde{D}}
\newcommand{\bu}{\textbf{u}}
\newcommand{\bU}{\textbf{U}}
\newcommand{\bV}{\textbf{V}}
\newcommand{\bb}{\textbf{\emph{b}}}
\newcommand{\bI}{\textbf{I}}
\newcommand{\be}{\boldsymbol{\varepsilon}}
\newcommand{\bSigma}{\boldsymbol{\Sigma}}
\newcommand{\bLambda}{\boldsymbol{\Lambda}}
\newcommand{\bTheta}{\boldsymbol{\Theta}}
\newcommand{\mb}[1]{\mathbf{#1}}
\newcommand {\bs}{\boldsymbol}
%\newcommand{\norm}[1]{\left\Vert #1 \right\Vert}
\newcommand{\xf}{\mathcal{X}}
\newcommand{\pfrac}[2]{\left( \frac{#1}{#2}\right)}
\newcommand{\e}{{\mathsf E}}
\newcommand{\bt}{\boldsymbol{\theta}}
\newcommand{\bmu}{\boldsymbol{\mu}}
\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\bbk}{\boldsymbol{\beta}_{(k)}}
\newcommand{\bbkt}{\widetilde{\boldsymbol{\beta}}_{(k)}}
\newcommand{\bPhi}{\boldsymbol{\Phi}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\diag}{diag} % operator and subscript

\DeclarePairedDelimiter\abs{\lvert}{\rvert}%
\DeclarePairedDelimiter\norm{\lVert}{\rVert}%

% Swap the definition of \abs* and \norm*, so that \abs
% and \norm resizes the size of the brackets, and the
% starred version does not.
\makeatletter
\let\oldabs\abs
\def\abs{\@ifstar{\oldabs}{\oldabs*}}
%
\let\oldnorm\norm
\def\norm{\@ifstar{\oldnorm}{\oldnorm*}}
\makeatother

%########################################################################################
%            						FANCY HEADER STUFF
%########################################################################################
\usepackage{lastpage}
\usepackage{fancyhdr}
\cfoot{\thepage}
\lhead[\leftmark]{}
\rhead[]{\leftmark}
\makeatletter
\makeatother
\lfoot{} \cfoot{ } \rfoot{{\small{\em Page \thepage \ of \pageref{LastPage}}}}


%########################################################################################
%            						SPACING
%########################################################################################

\usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent
%\usepackage[left=.1in,right=.1in,top=.1in,bottom=.1in]{geometry}
\usepackage[margin=1in]{geometry}
\usepackage[doublespacing]{setspace}
%\doublespacing


%########################################################################################
%            						TITLE and AUTHORS
%########################################################################################

%\title{A General Framework for Variable Selection in Linear Mixed Models with Applications to Genetic Studies with Structured Populations}
\title{Simultaneous SNP selection and adjustment for population structure in high dimensional prediction models}


\author[1,2]{Sahir R Bhatnagar}
\author[4]{Yi Yang}
\author[2]{Tianyuan Lu}
\author[6]{Erwin Schurr}
\author[7]{\mbox{JC Loredo-Osti}}
\author[2]{\mbox{Marie Forest}}
\author[3]{Karim Oualkacha}
\author[1,2,5]{\mbox{Celia MT Greenwood}}
\affil[1]{Department of Epidemiology, Biostatistics and Occupational Health, McGill University}
\affil[2]{Lady Davis Institute, Jewish General Hospital, Montr\'{e}al, QC}
\affil[3]{Département de Mathématiques, Université de Québec À Montréal}
\affil[4]{Department of Mathematics and Statistics, McGill University}
\affil[5]{Departments of Oncology and Human Genetics, McGill University}
\affil[6]{Department of Medicine, McGill University}
\affil[7]{Department of Mathematics and Statistics, Memorial University}
%########################################################################################
%            						START OF DOCUMENT
%########################################################################################
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}







\maketitle
\pagestyle{fancy}


\begin{abstract}
	Complex traits are known to be influenced by a combination of environmental factors and rare and common genetic variants. However, detection of such multivariate associations can be compromised by low statistical power and confounding by population structure. Linear mixed effects models (LMM) can account for correlations due to relatedness but have not been applicable in high-dimensional (HD) settings where the number of fixed effect predictors greatly exceeds the number of samples. False positives or false negatives can result from two-stage approaches, where the residuals estimated from a null model adjusted for the subjects' relationship structure are subsequently used as the response in a standard penalized regression model. To overcome these challenges, we develop a general penalized LMM with a single random effect called \ggmix ~for simultaneous SNP selection and adjustment for population structure in high dimensional prediction models. \textbf{We develop a blockwise coordinate descent algorithm with automatic tuning parameter selection which is highly scalable, computationally efficient and has theoretical guarantees of convergence. Through simulations and three real data examples, we show that \ggmix ~leads to more parsimonious models compared to the two-stage approach or principal component adjustment with better prediction accuracy. Our method performs well even in the presence of highly correlated markers, and when the causal SNPs are included in the kinship matrix. \ggmix ~can be used to construct polygenic risk scores and select instrumental variables in Mendelian randomization studies.} %We apply our method to identify SNPs that predict bone mineral density in the UK Biobank cohort.
	%This approach can also be used to generate genetic risk scores and finding groups of predictors associated with the response, such as variants within a gene or pathway.
	Our algorithms are available in an R package (\url{https://github.com/greenwoodlab/ggmix}).
\end{abstract}

\section{Author Summary}
This work addresses a recurring challenge in the analysis and interpretation of genetic association studies: which genetic variants can best predict and are independently associated with a given phenotype in the presence of population structure ? Not controlling confounding due to geographic population structure, family and/or cryptic relatedness can lead to spurious associations. Much of the existing research has therefore focused on modeling the association between a phenotype and a single genetic variant in a linear mixed model with a random effect. However, this univariate approach may miss true associations due to the stringent significance thresholds required to reduce the number of false positives and also ignores the correlations between markers. We propose an alternative method for fitting high-dimensional multivariable models, which selects SNPs that are independently associated with the phenotype while also accounting for population structure. We provide an efficient implementation of our algorithm and show through simulation studies and real data examples that our method outperforms existing methods in terms of prediction accuracy and controlling the false discovery rate. 
%single nucleotide polymorphisms (SNPs)
\section{Introduction}

Genome-wide association studies (GWAS) have become the standard method for analyzing genetic datasets owing to their success in identifying thousands of genetic variants associated with complex diseases (\url{https://www.genome.gov/gwastudies/}).
Despite these impressive findings, the discovered markers have only been able to explain a small proportion of the phenotypic variance; this is known as the missing heritability problem~\citep{manolio2009finding}.
One plausible reason is that there are many causal variants that each explain a small amount of variation with small effect sizes~\citep{yang2010common}.
Methods such GWAS, which test each variant or single nucleotide polymorphism (SNP) independently, may miss these true associations due to the stringent significance thresholds required to reduce the number of false positives~\citep{manolio2009finding}.
Another major issue to overcome is that of confounding due to geographic population structure, family and/or cryptic relatedness which can lead to spurious associations~\citep{astle2009population}.
For example, there may be subpopulations within a study that differ with respect to their genotype frequencies at a particular locus due to geographical location or their ancestry.
This heterogeneity in genotype frequency can cause correlations with other loci and consequently mimic the signal of association even though there is no biological association~\citep{song2015testing,marchini2004effects}.
Studies that separate their sample by ethnicity to address this confounding suffer from a loss in statistical power due to the drop in sample size.
%Furthermore, statistical power decreases if the error includes these extra sources of variation.


To address the first problem, multivariable regression methods have been proposed which simultaneously fit many SNPs in a single model~\citep{hoggart2008simultaneous,li2010bayesian}. Indeed, the power to detect an association for a given SNP may be increased when other causal SNPs have been accounted for. Conversely, a stronger signal from a causal SNP may weaken false signals when modeled jointly~\citep{hoggart2008simultaneous}.

Solutions for confounding by population structure have also received significant attention in the literature~\citep{lippert2011fast,kang2010variance,yu2006unified,eu2014comparison}.
There are two main approaches to account for the relatedness between subjects: 1) the principal component (PC) adjustment method and 2) the linear mixed model (LMM).
The PC adjustment method includes the top PCs of genome-wide SNP genotypes as additional covariates in the model~\citep{price2006principal}.
The LMM uses an estimated covariance matrix from the individuals' genotypes and includes this information in the form of a random effect~\cite{astle2009population}.

While these problems have been addressed in isolation, there has been relatively little progress towards addressing them jointly at a large scale.
Region-based tests of association have been developed where a linear combination of $p$ variants is regressed on the response variable in a mixed model framework~\citep{oualkacha2013adjusted}.
In case-control data, a stepwise logistic-regression procedure was used to evaluate the relative importance of variants within a small genetic region~\citep{cordell2002unified}.
These methods however are not applicable in the high-dimensional setting, i.e., when the number of variables $p$ is much larger than the sample size $n$, as is often the case in genetic studies where millions of variants are measured on thousands of individuals.

There has been recent interest in using penalized linear mixed models, which place a constraint on the magnitude of the effect sizes while controlling for confounding factors such as population structure.
For example, the LMM-lasso~\citep{rakitsch2013lasso} places a Laplace prior on all main effects while the adaptive mixed lasso~\citep{wang2011identifying} uses the $L_1$ penalty~\citep{tibshirani1996regression} with adaptively chosen weights~\citep{zou2006adaptive} to allow for differential shrinkage amongst the variables in the model.
Another method applied a combination of both the lasso and group lasso penalties in order to select variants within a gene most associated with the response~\citep{ding20142}.
However, methods such as the LMM-lasso are normally performed in two steps. First, the variance components are estimated once from a LMM with a single random effect. These LMMs normally use the estimated covariance matrix from the individuals' genotypes to account for the relatedness but assumes no SNP main effects (i.e. a null model).
The residuals from this null model with a single random effect can be treated as independent observations because the relatedness has been effectively removed from the original response.
In the second step, these residuals are used as the response in any high-dimensional model that assumes uncorrelated errors.
This approach has both computational and practical advantages since existing penalized regression software such as \texttt{glmnet}~\citep{friedman2010regularization} and \texttt{gglasso}~\citep{yang2015fast}, which assume independent observations, can be applied directly to the residuals.
However, recent work has shown that there can be a loss in power if a causal variant is included in the calculation of the covariance matrix as its effect will have been removed in the first step~\citep{oualkacha2013adjusted,yang2014advantages}.

In this paper we develop a general penalized LMM framework called \texttt{ggmix} that simultaneously selects variables and estimates their effects, accounting for between-individual correlations. We develop a blockwise coordinate descent algorithm with automatic tuning parameter selection which is highly scalable, computationally efficient and has theoretical guarantees of convergence. Our method can handle several sparsity inducing penalties such as the lasso~\citep{tibshirani1996regression} and elastic net~\citep{zou2005regularization}. Through simulations and three real data examples, we show that \ggmix ~leads to more parsimonious models compared to the two-stage approach or principal component adjustment with better prediction accuracy. Our method performs well even in the presence of highly correlated markers, and when the causal SNPs are included in the kinship matrix. 
%When the matrix of genotypes used to construct the covariance matrix is low rank, there are additional computational speedups that can be implemented.
%While this has been developed for the univariate case~\citep{lippert2011fast}, to our knowledge, this has not been explored in the multivariable case.
%The LMM-lasso paper mentions that this is possible but does not provide further details on how this can be implemented in a penalized mixed model framework.
%In the sequel, we develop a low rank version of the blockwise coordinate descent algorithm which reduces the time complexity from $\mathcal{O}(n^2k)$ to $\mathcal{O}(nk^2)$.
All of our algorithms are implemented in the \texttt{ggmix} R package hosted on GitHub with extensive documentation (\url{https://sahirbhatnagar.com/ggmix}). We provide a brief demonstration of the \texttt{ggmix} package in Appendix~\ref{ap:showcase}.

The rest of the paper is organized as follows. In Section 3, we compare the performance of our proposed approach and demonstrate the scenarios where it can be advantageous to use over existing methods through simulation studies and three real data analyses. This is followed by a discussion of our results, some limitations and future directions in Section 4. Section 5 describes the \texttt{ggmix} model, the optimization procedure and the algorithm used to fit it.


%~\citep{wang2011identifying,rakitsch2013lasso}. In both of these approaches, the lasso penalty~\citep{tibshirani1996regression} is used resulting in simultaneous estimation and selection of SNP effects.

%The multi-locus mixed model Approximate (2-step), stepwise mixed-model regression with forward inclusion and backward elimination.

%These appraoches are done in two steps where the null model, i.e., no snp effects is used to estimate the variance components. The snp effects are then estimated treating the variance estimates as fixed.

%MLMM~\citep{segura2012efficient} and LMM-Lasso~\citep{rakitsch2013lasso} regress one trait (or phenotype) against multiple predictors (e.g. SNPs) while accounting for the population structure. Neither GCAT~\citep{song2015testing} nor QTCAT~\citep{klasen2016multi} use mixed models. GCAT uses an inverse regression approach coupled with logistic factor analysis. QTCAT doesn't account for population structure, but instead searches for groups of highly correlated markers that are associated with the phenotype. ~\citep{wang2011identifying}


%while accounting for the population structure~\citep{hoggart2008simultaneous,wang2011identifying,song2015testing,rakitsch2013lasso}.



%then talk about how this doesnt work when p>n. Talk about lasso In addition to association testing, there are several other motivations for these models. Motivations include fine mapping, mendel randomization,


%\subsection{Measures of Relatedness}




%"Although the exact genetic relationships between individuals in the samples are unknown, we could take advantage of the high-density genotype information to empirically estimate the level of relatedness between reportedly unrelated individuals~\citep{kang2010variance}."






%see \url{http://dalexander.bol.ucla.edu/preprints/admixture-preprint.pdf} for details about confounding by population structure: ``Cluster analysis directly seeks the ancestral clusters in the data,	while principal component analysis (PCA) constructs low-dimensional projections of the	data that explain the gross variation in marker genotypes, which in practice is the variation between populations''


%In Table~\ref{tab:review} we outline existing \emph{multivariate} methods for genetic data containing related samples. MLMM~\citep{segura2012efficient} and LMM-Lasso~\citep{rakitsch2013lasso} regress one trait (or phenotype) against multiple predictors (e.g. SNPs) while accounting for the population structure. Neither GCAT~\citep{song2015testing} nor QTCAT~\citep{klasen2016multi} use mixed models. GCAT uses an inverse regression approach coupled with logistic factor analysis. QTCAT doesn't account for population structure, but instead searches for groups of highly correlated markers that are associated with the phenotype.

%Note that there is confusion in the genetics literature on the meaning of multivariate linear mixed models. For example, GEMMA~\citep{zhou2014efficient} is an association method for multiple traits against a single SNP, but is referred to in their paper as a ``multivariate mixed model''. MTMM~\citep{korte2012mixed} is also an association method for multiple phenotypes against a single SNP but is referred to as a ``multi-trait mixed model''.

%See the review by~\citep{eu2014comparison} for comparison of \emph{single} locus methods accounting for relatedness in GWAS with family data.

\begin{comment}
	\ctable[caption={Existing multivariate (multi-locus) methods for genetic data containing related samples.},label=tab:review,pos=h!,doinside=\footnotesize]{LLLLL}{
	}{
	\FL
	Method              			& Software   & Description \ML
	Multi-locus mixed-model (MLMM)~\citep{segura2012efficient}  & \href{https://github.com/Gregor-Mendel-Institute/mlmm}{R package on Github}   & \multicolumn{1}{m{9cm}}{Approximate (2-step), stepwise mixed-model regression with forward
		inclusion and backward elimination. Since variance attributed to random polygenic term decreases when cofactors are added to the model, heritable variance estimate as a criterion to stop forward inclusion. Association testing.}\\
	\addlinespace[5pt]
	LMM-Lasso~\citep{rakitsch2013lasso}  & \href{https://github.com/BorgwardtLab/LMM-Lasso}{Python code on Github} &  \multicolumn{1}{m{9cm}}{Approximate (2-step), Laplacian shrinkage prior over the fixed effects. Optimize $\delta = \sigma^2_e / \sigma^2_g$. Stability selection used to assess significance }\\
	\addlinespace[5pt]\\
	GCAT~\citep{song2015testing} & \href{https://github.com/StoreyLab/gcatest}{R package on Github} &   \multicolumn{1}{m{9cm}}{Inverse regression approach where the association is tested by modeling
		genotype variation in terms of the trait plus model terms accounting
		for structure. The terms accounting for structure were
		based on the logistic factor analysis~\citep{2013arXiv1312.2041H} approach}
	\\
	\addlinespace[5pt]\\
	QTCAT~\citep{klasen2016multi} &  \href{https://github.com/QTCAT/qtcat/}{R Package on Github} & \multicolumn{1}{m{9cm}}{Quantitative Trait Cluster Association Test. Do not account for population structure but instead search for clusters of highly correlated markers that are significantly associated to the phenotype using a hierarchical testing procedure for correlated covariates~\citep{meinshausen2008hierarchical}}
	\LL
}
\end{comment}

\FloatBarrier

%{\color{red} CG:  Is it fair to say that LMM-Lasso is the only one that allows for penalization?   Also - I don't know if Karim mentioned this - Jianping/Karim/Lajmi and I are developing another approach as well.  No penalization though. Manuscript due (for a special edition of CJS) by May.}


\section{Results}\label{sec:results}

In this section we demonstrate the performance of \texttt{ggmix} in a simulation study and three real data applications. 
%results should be written in past tense for plos genetics
\subsection{Simulation Study} \label{simustudy}

We evaluated the performance of \texttt{ggmix} in a variety of simulated scenarios. For each simulation scenario we compared \texttt{ggmix} to the \texttt{lasso} and the \texttt{twostep} method. For the \texttt{lasso}, we included the top 10 principal components from the simulated genotypes used to calculate the kinship matrix as unpenalized predictors in the design matrix. For the \texttt{twostep} method, we first fitted an intercept only model with a single random effect using the average information restricted maximum likelihood (AIREML) algorithm~\citep{gilmour1995average} as implemented in the \texttt{gaston} R package~\citep{gaston}. The residuals from this model were then used as the response in a regular \texttt{lasso} model. Note that in the \texttt{twostep} method, we removed the kinship effect in the first step and therefore did not need to make any further adjustments when fitting the penalized model. We fitted the \texttt{lasso} using the default settings and \texttt{standardize=FALSE} in the \texttt{glmnet} package~\citep{friedman2010regularization},\textbf{ with 10-fold cross-validation (CV) to select the optimal tuning parameter}. \textbf{For other parameters in our simulation study, we defined the following quantities:}

\begin{itemize}
	\item $n$: sample size
	\item $c$: percentage of causal SNPs
	%\item $p_{fixed}$: number of fixed effects
	\item $\bbeta$: true effect size vector of length $p$ 
	\item $S_0 = \left\lbrace j; (\bbeta)_j \neq 0 \right\rbrace$ the index of the true active set with cardinality $ |S_0| = c\times p$
	
	%\item $\beta_j^0$: true effect size for the $j^{th}$ SNP, simulated from a $Normal(0,1)$ distribution for $j \in S_0$ where $S_0 = \left\lbrace j; \beta_j^0 \neq 0 \right \rbrace$ is the index of the active set 
	%\item $\rho$: linkage disequilibrium between two SNPs
	
	\item \textbf{\textit{causal}: the list of causal SNP indices}
	\item \textbf{\textit{kinship}: the list of SNP indices for the kinship matrix}
	\item $\bX$: $n \times p$ \textbf{matrix of SNPs that were included as covariates in the model}
	%\item $\bX^{(causal)}$: $n \times |S_0|$ matrix of SNPs that were truly associated with the simulated phenotype, where $\bX^{(causal)} \subseteq \bX^{(fixed)}$
	%\item $\bX^{(other)}$: $n \times p_{other}$ matrix of SNPs that were used in the construction of the kinship matrix. Some of these $\bX^{(other)}$ SNPs, in conjunction with some of the SNPs in $\bX^{(fixed)}$ were used in construction of the kinship matrix. We altered the balance between these two contributors and with the proportion of causal SNPs used to calculate kinship
	%\item $\bX^{(kinship)}$: $n \times k$ matrix of SNPs used to construct the kinship matrix
	
	%\item $Y^* = \sum_{j=1}^{c*1000} \beta_j \bX^{(causal)}_j$
	%\item $Y = Y^* + k \cdot \varepsilon$, where the error term $\varepsilon$ is generated from a standard normal distribution, and $k$ is chosen such that the signal-to-noise ratio $SNR =\left(Var(Y^*)/Var(\varepsilon)\right)$ is 1
\end{itemize}

We simulated data from the model
\begin{equation}
\bY = \bX \bbeta + \mathbf{P} + \be
\end{equation}
where $\mathbf{P}\sim \mathcal{N}(0, \eta \sigma^2 \bPhi)$ is the polygenic effect and $\be \sim \mathcal{N}(0, (1-\eta) \sigma^2 \bI)$ is the error term. \textbf{Here, $\bPhi_{n \times n}$ is the covariance matrix based on the \textit{kinship} SNPs from $n$ individuals}, $\bI_{n \times n}$ is the identity matrix and parameters $\sigma^2$ and $\eta \in [0,1]$ determine how the variance is divided between $\mathbf{P}$ and $\be$. The values of the parameters that we used were as follows: narrow sense heritability $\eta=\lbrace 0.1, 0.3 \rbrace$, number of covariates $p = 5,000$, number of \textit{kinship} SNPs $k = 10,000$, percentage of \textit{causal} SNPs $c=\lbrace 0\%, 1\%\rbrace$ and $\sigma^2 = 1$.\textbf{ In addition to these parameters, we also varied the amount of overlap between the \textit{causal} list and the \textit{kinship} list. We considered two main scenarios:}

\begin{enumerate}
	\item \textbf{None of the \textit{causal} SNPs are included in \textit{kinship} set.}
	\item \textbf{All of the \textit{causal} SNPs are included in the \textit{kinship} set.}
\end{enumerate}

Both kinship matrices were meant to contrast the model behavior when the causal SNPs are included in both the main effects and random effects (referred to as proximal contamination~\citep{lippert2011fast}) versus when the causal SNPs are only included in the main effects. These scenarios are motivated by the current standard of practice in GWAS where the candidate marker is excluded from the calculation of the kinship matrix~\citep{lippert2011fast}. This approach becomes much more difficult to apply in large-scale multivariable models where there is likely to be overlap between the variables in the design matrix and kinship matrix. We simulated random genotypes from the BN-PSD admixture model with 1D geography and 10 subpopulations using the \texttt{bnpsd} package~\citep{bnpsd1,bnpsd2}. In Figure~\ref{fig:plot-kinship-sim}, we plot the estimated kinship matrix from a single simulated dataset in the form of a heatmap where a darker color indicates a closer genetic relationship. 

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=1\linewidth]{figure/plot-kinship-sim-1} 

}

\caption[Example of an empirical kinship matrix used in simulation studies]{Example of an empirical kinship matrix used in simulation studies. This scenario models a 1D geography with extensive admixture.}\label{fig:plot-kinship-sim}
\end{figure}


\end{knitrout}

In Figure~\ref{fig:plot-pc-sim} we plot the first two principal component scores calculated from the simulated genotypes used to calculate the kinship matrix in Figure~\ref{fig:plot-kinship-sim}, and color each point by subpopulation membership. We can see that the PCs can identify the subpopulations which is why including them as additional covariates in a regression model has been considered a reasonable approach to control for confounding.

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=1\linewidth]{figure/plot-pc-sim-1} 

}

\caption[First two principal component scores of the genotype data used to estimate the kinship matrix where each color represents one of the 10 simulated subpopulations]{First two principal component scores of the genotype data used to estimate the kinship matrix where each color represents one of the 10 simulated subpopulations.}\label{fig:plot-pc-sim}
\end{figure}


\end{knitrout}

Using this set-up, we randomly partitioned 1000 simulated observations into 80\% for training and 20\% for testing. The training set was used to fit the model and select the optimal tuning parameter only, and the resulting model was evaluated on the test set. Let $\hat{\lambda}$ be the estimated value of the optimal regularization parameter, $\widehat{\bbeta}_{\hat{\lambda}}$ the estimate of $\bbeta$ at regularization parameter $\hat{\lambda}$, and $\widehat{S}_{\hat{\lambda}} = \left\lbrace j; (\widehat{\bbeta}_{\hat{\lambda}})_j \neq 0 \right\rbrace$ the index of the set of non-zero estimated coefficients. \textbf{To compare the methods in the context of true positive rate (TPR), we selected the largest tuning parameter that would result in a false positive rate (FPR) closest to 5\%, but not more.} We also compared the model size ($|\widehat{S}_{\hat{\lambda}}|$), test set prediction error based on the refitted unpenalized estimates for each selected model, the estimation error ($||\widehat{\bbeta} - \bbeta||_2^2$), and the variance components ($\eta, \sigma^2$) for the polygenic random effect and error term. %The following estimator is used for the error variance of the lasso~\citep{reid2016study}:
%\begin{equation}
%\frac{1}{n - |\widehat{S}_{\hat{\lambda}}|} \norm{\bY - \bX \widehat{\bbeta}_{\hat{\lambda}}}_2^2
%\end{equation}

%\subsection{Results}



\textbf{The results are summarized in Table~\ref{tab:print-sim-table}. We see that \ggmix ~outperformed the \texttt{twostep} in terms of TPR, and was comparable to the \texttt{lasso}. This was the case, regardless of true heritability and whether the causal SNPs were included in the calculation of the kinship matrix. For the \texttt{twostep} however, the TPR at a FPR of 5\%, drops, on average, from 0.84 (when causal SNPs are not in the kinship) to 0.76 (when causal SNPs are in the kinship)}. Across all simulation scenarios, \texttt{ggmix} had the smallest estimation error, and smallest root mean squared prediction error (RMSE) on the test set while also producing the most parsimonious models. Both the \texttt{lasso} and \texttt{twostep} selected more false positives, even in the null model scenario. Both the \texttt{twostep} and \ggmix ~overestimated the heritability though \ggmix ~was closer to the true value. When none of the causal SNPs were in the kinship, both methods tended to overestimate the truth when $\eta=10\%$ and underestimate when $\eta=30\%$. Across all simulation scenarios \texttt{ggmix} was able to (on average) correctly estimate the error variance. The \texttt{lasso} tended to overestimate $\sigma^2$ in the null model while the \texttt{twostep} overestimated $\sigma^2$ when none of the causal SNPs were in the kinship matrix. 

Overall, we observed that variable selection results and RMSE for \ggmix ~were similar regardless of whether the causal SNPs were in the kinship matrix or not.
This result is encouraging since in practice the kinship matrix is constructed from a random sample of SNPs across the genome, some of which are likely to be causal, particularly in polygenic traits. %However, as pointed out by a reviewer, Yang et al.~\citep{yang2014advantages} showed that proximal contamination is not likely to be an issue 


%\ggmix ~had very good Type 1 and II error control, while both the \texttt{lasso} and \texttt{twostep} had a very high false positive rate in all simulation scenarios. 


In particular, our simulation results show that the principal component adjustment method may not be the best approach to control for confounding by population structure, particularly when variable selection is of interest. 


%Inclusion of the causal SNPs in the kinship calculation had the strongest impact on the variance component estimation with the heritabilty and error variance estimates working in opposite directions.
%That is, when all causal SNPs were in the kinship matrix, the heritability estimates were biased towards 1 while the error variance was correctly estimated.
%Conversely, when none of the causal SNPs were included in the kinship matrix, the estimated heritability was closer to the true value, while the error variance was inflated. 
%The imprecision of the variance component estimation however did not impact the performance of \ggmix ~in terms of selecting the true causal SNPs and prediction error; this had a much more negative impact on the \texttt{twostep} method which selected many false positives and had higher RMSE.





%Both the \texttt{lasso} and \texttt{twostep} methods have better signal recovery as compared to \ggmix. However, this signal is being spread across many variables leading to many Type 1 errors.

%In Figures~\ref{fig:plot-errorvar-sim-null-model} (Supplemental Section~\ref{ap:ggmix_sim_results}) and~\ref{fig:plot-errorvar-sim-1p-causal}, we plot the error variance for $c=0$ and $c=0.01$, respectively. The \texttt{twostep} and \ggmix ~methods correctly estimate the error variance while the \texttt{lasso} overestimates it for the null model and for when 1\% of the causal SNPs are in the kinship matrix. We see an inflated estimated error variance across all three methods when $c=0.01$ and none of the causal SNPs are in the kinship matrix with the \texttt{lasso} and \ggmix ~performing similarly.

%methods sometimes estimate a model with a large number of false positives. When the true model contains some causal SNPs, \ggmix ~again outperforms the other two methods in terms of correct sparsity. The distribution of $\widehat{S}_{\hat{\lambda}}$ for each of the three methods is shown in Figure~\ref{fig:plot-nactive-sim-null-model} for $c=0$ and Figure~\ref{fig:plot-nactive-sim-1p-causal} for $c=0.01$ of Supplemental Section~\ref{ap:ggmix_sim_results}.







\begin{table}

\caption{\label{tab:print-sim-table}Mean (standard deviation) from 200 simulations stratified by the number of causal SNPs (null, 1\%), the overlap between causal SNPs and kinship matrix (no overlap, all causal SNPs in kinship), and true heritability (10\%, 30\%).
                  For all simulations, sample size is $n=1000$, the number of covariates is $p=5000$, and the number of SNPs used to estimate the kinship matrix is $k=10000$.
                  TPR at FPR=5\% is the true positive rate at a fixed false positive rate of 5\%.
                  Model Size ($|\widehat{S}_{\hat{\lambda}}|$) is the number of selected variables in the training set using the high-dimensional BIC for \texttt{ggmix} and 10-fold cross validation for \texttt{lasso} and \texttt{twostep}.
                  RMSE is the root mean squared error on the test set.
                  Estimation error is the squared distance between the estimated and true effect sizes.
                  Error variance ($\sigma^2$) for \texttt{twostep} is estimated from an intercept only LMM with a single random effect and is modeled explicitly in \ggmix. For the \texttt{lasso} we use $\protect\frac{1}{n - |\widehat{S}_{\hat{\lambda}}|} \protect||\bY - \bX \widehat{\bbeta}_{\hat{\lambda}}||_2^2$~\citep{reid2016study} as an estimator for $\sigma^2$.
                  Heritability ($\eta$) for \texttt{twostep} is estimated as $\sigma_g^2 / (\sigma_g^2 + \sigma_e^2)$ from an intercept only LMM with a single random effect where $\sigma_g^2$ and $\sigma_e^2$ are the variance components for the random effect and error term, respectively. $\eta$ is explictly modeled in \ggmix. There is no positive way to calculate $\eta$ for the \texttt{lasso} since we are using a PC adjustment.}
\centering
\fontsize{7}{9}\selectfont
\begin{tabu} to \linewidth {>{\bfseries}l>{\raggedright}X>{\centering}X>{\centering}X>{\centering}X>{\centering}X>{\centering}X>{\centering}X>{\centering}X>{\centering}X}
\toprule
\multicolumn{1}{c}{ } & \multicolumn{1}{c}{ } & \multicolumn{4}{c}{Null model} & \multicolumn{4}{c}{1\% Causal SNPs} \\
\cmidrule(l{3pt}r{3pt}){3-6} \cmidrule(l{3pt}r{3pt}){7-10}
\multicolumn{1}{c}{ } & \multicolumn{1}{c}{ } & \multicolumn{2}{c}{No overlap} & \multicolumn{2}{c}{\makecell[c]{All causal SNPs\\in kinship}} & \multicolumn{2}{c}{No overlap} & \multicolumn{2}{c}{\makecell[c]{All causal SNPs\\in kinship}} \\
\cmidrule(l{3pt}r{3pt}){3-4} \cmidrule(l{3pt}r{3pt}){5-6} \cmidrule(l{3pt}r{3pt}){7-8} \cmidrule(l{3pt}r{3pt}){9-10}
Metric & Method & 10\% & 30\% & 10\% & 30\% & 10\% & 30\% & 10\% & 30\%\\
\midrule
\rowcolor{gray!6}   & twostep & 0.00 (0.00) & 0.00 (0.00) & 0.00 (0.00) & 0.00 (0.00) & 0.84 (0.05) & 0.84 (0.05) & 0.76 (0.09) & 0.77 (0.08)\\

\rowcolor{gray!6}   & lasso & 0.00 (0.00) & 0.00 (0.00) & 0.00 (0.00) & 0.00 (0.00) & 0.86 (0.05) & 0.85 (0.05) & 0.86 (0.05) & 0.86 (0.05)\\

\rowcolor{gray!6}  \multirow{-3}{*}{\raggedright\arraybackslash TPR at FPR=5\%} & ggmix & 0.00 (0.00) & 0.00 (0.00) & 0.00 (0.00) & 0.00 (0.00) & 0.86 (0.05) & 0.86 (0.05) & 0.85 (0.05) & 0.86 (0.05)\\
\cmidrule{1-10}
 & twostep & 0 (0, 5) & 0 (0, 2) & 0 (0, 5) & 0 (0, 2) & 328 (289, 388) & 332 (287, 385) & 284 (250, 329) & 284 (253, 319)\\

 & lasso & 0 (0, 6) & 0 (0, 5) & 0 (0, 6) & 0 (0, 5) & 278 (246, 317) & 276 (245, 314) & 279 (252, 321) & 285 (244, 319)\\

\multirow{-3}{*}{\raggedright\arraybackslash Model Size} & ggmix & 0 (0, 0) & 0 (0, 0) & 0 (0, 0) & 0 (0, 0) & 43 (39, 49) & 43 (39, 48) & 44 (38, 49) & 43 (38, 48)\\
\cmidrule{1-10}
\rowcolor{gray!6}   & twostep & 1.02 (0.07) & 1.02 (0.06) & 1.02 (0.07) & 1.02 (0.06) & 1.42 (0.10) & 1.41 (0.10) & 1.44 (0.33) & 1.40 (0.22)\\

\rowcolor{gray!6}   & lasso & 1.02 (0.06) & 1.02 (0.06) & 1.02 (0.06) & 1.02 (0.06) & 1.39 (0.09) & 1.38 (0.09) & 1.40 (0.08) & 1.38 (0.08)\\

\rowcolor{gray!6}  \multirow{-3}{*}{\raggedright\arraybackslash RMSE} & ggmix & 1.00 (0.05) & 1.00 (0.05) & 1.00 (0.05) & 1.00 (0.05) & 1.22 (0.10) & 1.20 (0.10) & 1.23 (0.11) & 1.23 (0.12)\\
\cmidrule{1-10}
 & twostep & 0.12 (0.22) & 0.09 (0.19) & 0.12 (0.22) & 0.09 (0.19) & 2.97 (0.60) & 2.92 (0.60) & 3.60 (5.41) & 3.21 (3.46)\\

 & lasso & 0.13 (0.21) & 0.12 (0.22) & 0.13 (0.21) & 0.12 (0.22) & 2.76 (0.46) & 2.69 (0.47) & 2.82 (0.48) & 2.75 (0.48)\\

\multirow{-3}{*}{\raggedright\arraybackslash Estimation Error} & ggmix & 0.00 (0.01) & 0.01 (0.02) & 0.00 (0.01) & 0.01 (0.02) & 2.11 (1.28) & 2.04 (1.22) & 2.21 (1.24) & 2.28 (1.34)\\
\cmidrule{1-10}
\rowcolor{gray!6}   & twostep & 0.87 (0.11) & 0.69 (0.15) & 0.87 (0.11) & 0.69 (0.15) & 14.23 (3.53) & 14.13 (3.52) & 1.42 (1.71) & 1.28 (1.66)\\

\rowcolor{gray!6}   & lasso & 0.98 (0.05) & 0.96 (0.05) & 0.98 (0.05) & 0.96 (0.05) & 1.04 (0.13) & 1.02 (0.13) & 1.03 (0.14) & 1.01 (0.14)\\

\rowcolor{gray!6}  \multirow{-3}{*}{\raggedright\arraybackslash Error Variance} & ggmix & 0.85 (0.18) & 0.64 (0.20) & 0.85 (0.18) & 0.64 (0.20) & 2.00 (0.49) & 1.86 (0.51) & 1.06 (0.46) & 0.83 (0.45)\\
\cmidrule{1-10}
 & twostep & 0.13 (0.11) & 0.31 (0.15) & 0.13 (0.11) & 0.31 (0.15) & 0.26 (0.14) & 0.26 (0.14) & 0.92 (0.08) & 0.93 (0.08)\\

 & lasso & -- & -- & -- & -- & -- & -- & -- & --\\

\multirow{-3}{*}{\raggedright\arraybackslash Heritability} & ggmix & 0.15 (0.18) & 0.37 (0.21) & 0.15 (0.18) & 0.37 (0.21) & 0.18 (0.16) & 0.23 (0.17) & 0.59 (0.20) & 0.68 (0.19)\\
\bottomrule
\multicolumn{10}{l}{\textit{Note:}}\\
\multicolumn{10}{l}{Median (Inter-quartile range) is given for Model Size.}\\
\end{tabu}
\end{table}






%The true positive vs. false positive rate for the model with 1\% causal SNPs ($c=0.01$) is shown in Figure~\ref{fig:plot-tpr-fpr-sim-1p-causal}. B






%We compare the model error as a function of $\widehat{S}_{\hat{\lambda}}$ in Figures~\ref{fig:plot-me-nactive-sim-null} (Supplemental Section~\ref{ap:ggmix_sim_results}) and~\ref{fig:plot-me-nactive-sim-1p-causal} for $c=0$ and $c=0.01$, respectively. \texttt{lasso} achieves the smallest model error across all scenarios (for $c=0.01$), albeit with a large number of active variables. \ggmix ~has a smaller model error compared to \texttt{twostep} when all causal SNPs are in the kinship matrix and similar performance when none of the causal SNPs are in the kinship matrix.










\subsection{Real Data Applications}

Three datasets with different features were used to illustrate the potential advantages of ~\ggmix ~over existing approaches such as PC adjustment in a \texttt{lasso} regression. In the first two datasets, family structure induced low levels of correlation and sparsity in signals. In the last, a dataset involving mouse crosses, correlations were extremely strong and could confound signals.

\subsubsection{UK Biobank}

\textbf{With more than 500,000 participants, the UK Biobank is one of the largest genotyped health care registries in the world. Among these participants, 147,731 have been inferred to be related to at least one individual in this cohort~\citep{bycroft2018uk}. Such a widespread genetic relatedness may confound association studies and bias trait predictions if not properly accounted for. Among these related individuals, 18,150 have a documented familial relationship (parent-offspring, full siblings, second degree or third degree) that was previously inferred in~\citep{biobank2015genotyping}. We attempted to derive a polygenic risk score for height among these individuals. As suggested by a reviewer, the goal of this analysis was to see how the different methods performed for a highly polygenic trait in a set of related individuals. We compared the \ggmix-derived polygenic risk score to those derived by the \texttt{twostep} and \texttt{lasso} methods.}

\textbf{We first estimated the pairwise kinship coefficient among the 18,150 reportedly related individuals based on 784,256 genotyped SNPs using KING~\citep{manichaikul2010robust}. We grouped related individuals with a kinship coefficient > 0.044~\citep{manichaikul2010robust} into 8,300 pedigrees. We then randomly split the dataset into a training set, a model selection set and a test set of roughly equal sample size, ensuring all individuals in the same pedigree were assigned into the same set. We inverse normalized the standing height after adjusting for age, sex, genotyping array, and assessment center following Yengo et al.~\citep{yengo2018meta}. }

\textbf{To reduce computational complexity, we selected 10,000 SNPs with the largest effect sizes associated with height from a recent large meta-analysis~\citep{yengo2018meta}. Among these 10,000 SNPs, 1,233 were genotyped and used for estimating the kinship whereas the other 8,767 SNPs were imputed based on the Haplotype Reference Consortium reference panel~\citep{mccarthy2016reference}. The distribution of the 10,000 SNPs by chromosome and whether or not the SNP was imputed is shown in Figure~\ref{fig:UKB-chromosome-distribution} in Supplemental Section~\ref{ap:rda}. We see that every chromosome contributed SNPs to the model with 15\% coming from chromosome 6. The markers we used are theoretically independent since Yengo et al. performed a COJO analysis which should have tuned down signals due to linkage disequilibrium~\citep{yengo2018meta}. We used \ggmix, \texttt{twostep} and \texttt{lasso} to select SNPs most predictive of the inverse normalized height on the training set, and chose the $\lambda$ with the lowest prediction RMSE on the model selection set for each method. We then examined the performance of each derived polygenic risk score on the test set. Similar to Section~\ref{simustudy}, we adjusted for the top 10 genetic PCs as unpenalized predictors when fitting the \texttt{lasso} models, and supplied the kinship matrix based on 784,256 genotyped SNPs to \texttt{ggmix} and \texttt{twostep}.}

\textbf{We found that with a kinship matrix estimated using all genotyped SNPs, \ggmix ~had the possibility to achieve a lower RMSE on the model selection set compared to the \texttt{twostep} and \texttt{lasso} methods (Figure~\ref{fig:UKB-Figure}A). An optimized \ggmix-derived polygenic risk score that utilized the least number of SNPs was also able to better predict the trait with lower RMSE on the test set (Figure~\ref{fig:UKB-Figure}B).}

\textbf{We additionally applied a Bayesian Sparse Linear Mixed Model (\texttt{BSLMM})~\citep{zhou2013polygenic} implemented in the GEMMA package~\citep{zhou2012genome} to derive a polygenic risk score on the training set. \texttt{A posterior probability of inclusion of each SNP was provided and prediction was based on all SNPs with a positive posterior probability.} We found that although the \texttt{BSLMM}-based polygenic risk score leveraged the most SNPs, it did not achieve a comparable prediction accuracy as the other three methods (Figure~\ref{fig:UKB-Figure}B). \texttt{Likely due to the small effect sizes of these SNPs, only 94, 35 and 1 SNPs had a posterior inclusion probability above 0.05, 0.10 and 0.50, respectively. The model would have further reduced prediction accuracy if the prediction was based only on these SNPs.}}


\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=1\linewidth]{figure/UKB-Figure-1} 

}

\caption[Model selection and testing in the UK Biobank]{Model selection and testing in the UK Biobank. (A) Root-mean-square error of three methods on the model selection set with respect to a grid search of penalty factor used on the training set. (B) Performance of four methods on the test set with penalty factor optimized on the model selection set. The x-axis has a logarithmic scale. The BSLMM method optimized coefficients of each SNP through an MCMC process on the training set and was directly evaluated on the test set.}\label{fig:UKB-Figure}
\end{figure}


\end{knitrout}

\begin{comment}
\begin{knitrout}\scriptsize
	\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]
		
		{\centering \includegraphics[width=0.9\linewidth]{figure/UKB-Figure} 
			
		}
		
		\caption[Model selection and testing in the UK Biobank.]{Model selection and testing in the UK Biobank. (a) Root-mean-square error of three methods on the model selection set with respect to a grid search of penalty factor used on the training set. (b) Performance of four methods on the test set with penalty factor optimized on the model selection set. The x-axis has a logarithmic scale. The BSLMM method optimized coefficients of each SNP through an MCMC process on the training set and was directly evaluated on the test set.}\label{fig:UKB-Figure}
	\end{figure}
	
	
\end{knitrout}
\end{comment}


\subsubsection{GAW20}

In the most recent Genetic Analysis Workship 20 (GAW20), the causal modeling group investigated causal relationships between DNA methylation (exposure) within some genes and the change in high-density lipoproteins $\Delta$HDL (outcome) using Mendelian Randomization (MR)~\citep{davey2003mendelian}.
Penalized regression methods were used to select SNPs strongly associated with the exposure in order to be used as an instrumental variable (IV)~\citep{cherlin2018using,zhou2018analysis}.
However, since GAW20 data consisted of families, \texttt{twostep} methods were used which could have resulted in a large number of false positives or false negatives. \ggmix~now provides an alternative approach that could be used for selecting the IV while accounting for the family structure of the data.

We applied \ggmix ~to all 200 GAW20 simulation datasets, each of 679 observations, and compared its performance to the \texttt{twostep} and \texttt{lasso} methods.
Using a Factored Spectrally Transformed Linear Mixed Model (FaST-LMM)~\citep{howey2018application} adjusted for age and sex, we validated the effect of rs9661059 on blood lipid trait to be significant (genome-wide $p$ = \num{6.29e-9}).
Though several other SNPs were also associated with the phenotype, these associations were probably mediated by CpG-SNP interaction pairs and did not reach statistical significance.
Therefore, to avoid ambiguity, we only focused on chromosome 1 containing 51,104 SNPs, including rs9661059.
Given that population admixture in the GAW20 data was likely, we estimated the population kinship using REAP~\citep{thornton2012estimating} after decomposing population compositions using ADMIXTURE~\citep{alexander2009fast}. We used 100,276 LD-pruned whole-genome genotyped SNPs for estimating the kinship. Among these, 8100 were included as covariates in our models based on chromosome 1. The causal SNP was also among the 100,276 SNPs. All methods were fit according to the same settings described in our simulation study in Section~\ref{simustudy}, and adjusting for age and sex. We calculated the median (inter-quartile range) number of active variables, and RMSE (standard deviation) based on five-fold CV on each simulated dataset.

On each simulated replicate, we calibrated the methods so that they could be easily compared by fixing the true positive rate to 1 and then minimizing the false positive rate. Hence, the selected SNP, rs9661059, was likely to be the true positive for each method, and non-causal SNPs were excluded to the greatest extent.
All three methods precisely chose the correct predictor without any false positives in more than half of the replicates, as the causal signal was strong. However, when some false positives were selected (i.e. when the number of active variables > 1), \ggmix ~performed comparably to \texttt{twostep}, while the \texttt{lasso} was inclined to select more false positives as suggested by the larger third quartile number of active variables (Table~\ref{tab:GAW20-prediction-RMSE-activeVariable}).
We also observed that \ggmix ~outperformed the \texttt{twostep} method with lower CV RMSE using the same number of SNPs. Meanwhile, it achieved roughly the same prediction accuracy as \texttt{lasso} but with fewer non-causal SNPs (Table~\ref{tab:GAW20-prediction-RMSE-activeVariable}). \textbf{It is also worth mentioning that there was very little correlation between the causal SNP and SNPs within a 1Mb-window around it (Figure~\ref{fig:gaw20r2} in Supplemental Section~\ref{ap:ldplots}), making it an ideal scenario for the \texttt{lasso} and related methods.}

\textbf{We also applied the \texttt{BSLMM} method by performing five-fold CV on each of the 200 simulated replicates. 
	We found that while \texttt{BSLMM} achieved a lower CV RMSE compared to the other methods (Table~\ref{tab:GAW20-prediction-RMSE-activeVariable}), this higher prediction accuracy relied on approximately 80\% of the 51,104 SNPs \texttt{with a positive posterior probability}. 
	This may suggest overfitting in this dataset. 
	\texttt{On the other hand, we found that imposing a posterior inclusion probability threshold might improve feature selection, but the results were sensitive to the arbitrary threshold as most SNPs had a low posterior probability.} It is also noteworthy that we did not adjust for age and sex in the \texttt{BSLMM} model, as the current implementation of the method in the GEMMA package does not allow adjustment for covariates.}


\newcolumntype{C}[1]{>{\Centering}m{#1}}
\renewcommand\tabularxcolumn[1]{C{#1}}

\begin{minipage}{\linewidth}
	\bigskip
	\captionof{table}{Summary of model performance based on 200 GAW20 simulations. Five-fold cross-validation root-mean-square error was reported for each simulation replicate. Prediction accuracy for BSLMM model with a positive posterior inclusion probability threshold was not reported because the model could contain no SNP.} \label{tab:GAW20-prediction-RMSE-activeVariable}
	\begin{tabular}{ C{0.9in} C{3.5in} C{1.9in}}\toprule[1.5pt]
		\bf Method & \bf Median number of active variables (Inter-quartile range) & \bf RMSE (SD) \\\midrule
		\texttt{twostep} & 1 (1 - 11) & 0.3604 (0.0242) \\
		\texttt{lasso} & 1 (1 - 15) & 0.3105 (0.0199) \\
		\texttt{ggmix} & 1 (1 - 12) & 0.3146 (0.0210) \\
		\texttt{BSLMM} & 40,737 (39,901 - 41,539) & 0.2503 (0.0099)\\
		\texttt{BSLMM, posterior probability > 0.05} & 2 (1 - 4) & \\
		\texttt{BSLMM, posterior probability > 0.10} & 0 (0 - 1)  & \\
		\texttt{BSLMM, posterior probability > 0.50} & 0 (0 - 0)  & \\
		\bottomrule[1.25pt]
		\end {tabular}\par
		\bigskip
	\end{minipage}

\subsubsection{Mouse Crosses and Sensitivity to Mycobacterial Infection}

Mouse inbred strains of genetically identical individuals are extensively used in research.
Crosses of different inbred strains are useful for various studies of heritability focusing on either observable phenotypes or molecular mechanisms, and in particular, recombinant congenic strains have been an extremely useful resource for many years~\citep{fortin2001recombinant}.
However, ignoring complex genetic relationships in association studies can lead to inflated false positives in genetic association studies when different inbred strains and their crosses are investigated~\citep{bennett2010high,flint2012genome,cheng2010genome}.
Therefore, a previous study developed and implemented a mixed model to find loci associated with mouse sensitivity to mycobacterial infection~\citep{di2010strain}. The random effects in the model captured complex correlations between the recombinant congenic mouse strains based on the proportion of the DNA shared identical by descent. Through a series of mixed model fits at each marker, new loci that impact growth of mycobacteria on chromosome 1 and chromosome 11 were identified.

Here we show that \ggmix ~can identify these loci, as well as potentially others, in a single analysis. We reanalyzed the growth permissiveness in the spleen, as measured by colony forming units (CFUs), 6 weeks after infection from $Mycobacterium$ $bovis$ Bacille Calmette-Guerin (BCG) Russia strain as reported in ~\citep{di2010strain}.

By taking the consensus between the ``main model'' and the ``conditional model'' of the original study, we regarded markers D1Mit435 on chromosome 1 and D11Mit119 on chromosome 11 as two true positive loci.
We directly estimated the kinship between mice using genotypes at 625 microsatellite markers. The estimated kinship entered directly into \ggmix ~and \texttt{twostep}. For the \texttt{lasso}, we calculated and included the first 10 principal components of the estimated kinship. To evaluate the robustness of different models, we bootstrapped the 189-sample dataset and repeated the analysis 200 times. \textbf{We then conceived a two-fold criteria to evaluate performance of each model. We first examined whether a model could pick up both true positive loci using some $\lambda$. If the model failed to pick up both loci simultaneously with any $\lambda$, we counted as modeling failure on the corresponding boostrap replicate; otherwise, we counted as modeling success and recorded which other loci were picked up given the largest $\lambda$. Consequently, similar to the strategy used in the GAW20 analysis, we optimized the models by tuning the penalty factor such that these two true positive loci were picked up, while the number of other active loci was minimized.}
Significant markers were defined as those captured in at least half of the successful bootstrap replicates (Figure~\ref{fig:Mice-comparison-fixTPR}).

We demonstrated that \ggmix ~recognized the true associations more robustly than \texttt{twostep} and \texttt{lasso}. In almost all (99\%) bootstrap replicates, \ggmix ~was able to capture both true positives, while the \texttt{twostep} failed in 19\% of the replicates and the \texttt{lasso} failed in 56\% of the replicates by missing at least one of the two true positives (Figure~\ref{fig:Mice-comparison-fixTPR}). \textbf{The robustness of \ggmix ~is particularly noteworthy due to the strong correlations between all microsatellite markers in this dataset (Figure~\ref{fig:miceR2} in Supplemental Section~\ref{ap:ldplots}). These strong correlations with the causal markers, partially explain the poor performance of the \texttt{lasso} as it suffers from unstable selections in the presence of correlated variables (e.g.~\citep{wang2018precision}).} 


We also identified several other loci that might also be associated with susceptibility to mycobacterial infection (Table~\ref{tab:Marker}). Among these new potentially-associated markers, D2Mit156 was found to play a role in control of parasite numbers of $Leishmania$ $tropica$ in lymph nodes~\citep{sohrabi2013mapping}. An earlier study identified a parent-of-origin effect at D17Mit221 on CD4M levels~\citep{jackson1999multiple}. This effect was more visible in crosses than in parental strains. In addition, D14Mit131, selected only by \ggmix ~, was found to have a 9\% loss of heterozygosity in hybrids of two inbred mouse strains~\citep{c2000allelotype}, indicating the potential presence of putative suppressor genes pertaining to immune surveillance and tumor progression~\citep{lasko1991loss}. This result might also suggest association with anti-bacterial responses yet to be discovered. 

%\textbf{We did not apply the \texttt{BSLMM} method because the microsatellite marker-based genotypes could not be converted to a BIMBAM or PLINK format that the package demands.}

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=1\linewidth]{figure/Mice-comparison-fixTPR-1} 

}

\caption[Comparison of model performance on the mouse cross data]{Comparison of model performance on the mouse cross data. Pie charts depict model robustness where grey areas denote bootstrap replicates on which the corresponding model is unable to capture both true positives using any penalty factor, whereas colored areas denote successful replicates. Chromosome-based signals record in how many successful replicates the corresponding loci are picked up by the corresponding optimized model. Red dashed lines delineate significance thresholds.}\label{fig:Mice-comparison-fixTPR}
\end{figure}


\end{knitrout}

\newcolumntype{C}[1]{>{\Centering}m{#1}}
\renewcommand\tabularxcolumn[1]{C{#1}}

\begin{minipage}{\linewidth}
	\bigskip
	\captionof{table}{Additional loci significantly associated with mouce susceptibility to myobacterial infection, after excluding two true positives. Loci needed to be identified in at least 50\% of the successful bootstrap replicates that captured both true positive loci.} \label{tab:Marker}
	\begin{tabular}{ C{1.25in} C{.85in} C{1.25in} C{2.5in}}\toprule[1.5pt]
		\bf Method & \bf Marker & \bf Position in cM & \bf Position in bp\\\midrule
		\texttt{twostep} & N/A & N/A & N/A\\
		\hline
		\texttt{lasso} & D2Mit156 & Chr2:31.66 & Chr2:57081653-57081799\\
		& D14Mit155 & Chr14:31.52 & Chr14:59828398-59828596\\
		\hline
		ggmix & D2Mit156 & Chr2:31.66 & Chr2:57081653-57081799\\
		& D14Mit131 & Chr14:63.59 & Chr14:120006565-120006669\\
		& D17Mit221 & Chr17:59.77 & Chr17:90087704-90087842\\
		\bottomrule[1.25pt]
		\end {tabular}\par
		\bigskip
	\end{minipage}



\section{Discussion}\label{sec:discussion}

We have developed a general penalized LMM framework called \ggmix ~which simultaneously selects SNPs and adjusts for population structure in high dimensional prediction models. \textbf{We compared our method to the \texttt{twostage} procedure, where in the first stage, the dependence between observations is adjusted for in a LMM with a single random effect and no covariates (i.e. null model). The residuals from this null model can then be used in any model for independent observations because the relatedness has been effectively removed from the original response. We also compared our method to the \texttt{lasso} and \texttt{BSLMM} which are closely related to \texttt{ggmix} since they also jointly model the relatedness and SNPs in a single step. The key differences are that the \texttt{lasso} uses a principal component adjustment and \texttt{BSLMM} is a Bayesian method focused on phenotype prediction.}

Through an extensive simulation study and three real data analyses that mimic many experimental designs in genetics, we show that the current approaches of PC adjustment and two-stage procedures are not necessarily sufficient to control for confounding by population structure leading to a high number of false positives. Our simulation results show that \texttt{ggmix} outperforms existing methods in terms of sparsity and prediction error even when the causal variants are included in the kinship matrix (Table~\ref{tab:print-sim-table}). Many methods for single-SNP analyses avoid this proximal contamination~\citep{lippert2011fast} by using a leave-one-chromosome-out scheme~\citep{loh2015efficient}, i.e., construct the kinship matrix using all chromosomes except the one on which the marker being tested is located. \textbf{However, this approach is not possible if we want to model many SNPs (across many chromosomes) jointly to create, for example, a polygenic risk score. For the purposes of variable selection, we would also want to model all chromosomes together since the power to detect an association for a given SNP may be increased when other causal SNPs have been accounted for. Conversely, a stronger signal from a causal SNP may weaken false signals when modeled jointly~\citep{hoggart2008simultaneous}, particularly when the markers are highly correlated as in the mouse crosses example.}

%Furthermore, \ggmix ~showed improved prediction performance with a more parsimonious model compared to the competing methods. Indeed, in the UK Biobank example, we saw that \texttt{ggmix} achieved a lower test set prediction error with a more parsimonious model compared to \texttt{twostep}, \texttt{lasso} and \texttt{BSLMM}. In the GAW20 example, \texttt{ggmix} and \texttt{lasso} had better test set prediction performance compared to \texttt{twostep}. The \texttt{BSLMM} had the best RMSE, but relied on approximately 80\% of the SNPs to achieve such a performance. In the mouse crosses example, \texttt{ggmix} had superior variable selection results compared to both the \texttt{twostep} and \texttt{lasso}.
%Our proposed method has excellent Type 1 error control and is robust to the inclusion of causal SNPs in the kinship matrix. 
\textbf{In the UK Biobank, we found that with a kinship matrix estimated using all genotyped SNPs, \ggmix ~had achieved a lower RMSE on the model selection set compared to the \texttt{twostep} and \texttt{lasso} methods. Furthermore, an optimized \ggmix-derived polygenic risk score that utilized the least number of SNPs was also able to better predict the trait with lower RMSE on the test set.} 
In the GAW20 example, we showed that while all methods were able to select the strongest causal SNP, \ggmix ~did so with the least amount of false positives while also maintaining good predictive ability. In the mouse crosses example, we showed that \ggmix ~is robust to perturbations in the data using a bootstrap analysis. Indeed, \ggmix ~was able to consistently select the true positives across bootstrap replicates, while \texttt{twostep} failed in 19\% of the replicates and \texttt{lasso} failed in 56\% of the replicates by missing of at least one of the two true positives. Our re-analysis of the data also lead to some potentially new findings, not found by existing methods, that may warrant further study. \textbf{This particular example had many markers that were strongly correlated with each other (Figure~\ref{fig:miceR2} of Supplemental Section~\ref{ap:ldplots}). Nevertheless, we observed that the two true positive loci were the most often selected while none of the nearby markers were picked up in more than 50\% of the 200 bootstrap replicates. This shows that our method does recognize the true positives in the presence of highly correlated markers. Nevertheless, we think the issue of variable selection for correlated SNPs warrants further study. The recently proposed Precision Lasso~\citep{wang2018precision} seeks to address this problem in the high-dimensional fixed effects model.} 



%Our CGD algorithm is computationally efficient and has theoretical guarantees of convergence.
%We provide an easy-to-use software implementation of our algorithm along with a principled method for automatic tuning parameter selection.
%Through simulation studies, we show that existing approaches such as a two-stage approach or the \texttt{lasso} with a principal component adjustment lead to a large number of false positives.

We emphasize here that previously developed methods such as the LMM-lasso~\citep{rakitsch2013lasso} use a two-stage fitting procedure without any convergence details.
From a practical point of view, there is currently no implementation that provides a principled way of determining the sequence of tuning parameters to fit, nor a procedure that automatically selects the optimal value of the tuning parameter.
To our knowledge, we are the first to develop a coordinate gradient descent (CGD) algorithm in the specific context of fitting a penalized LMM for population structure correction with theoretical guarantees of convergence. Furthermore, we develop a principled method for automatic tuning parameter selection and provide an easy-to-use software implementation in order to promote wider uptake of these more complex methods by applied practitioners. 




Although we derive a CGD algorithm for the $\ell_1$ penalty, our approach can also be easily extended to other penalties such as the elastic net and group lasso with the same guarantees of convergence.
%To our knowledge, we are the first to develop a CGD algorithm in the specific context of fitting a penalized LMM for population structure correction with theoretical guarantees of convergence. Furthermore, we develop a principled method for automatic tuning parameter selection and provide an easy-to-use software implementation in order to promote wider uptake of these more complex methods by applied practitioners.
A limitation of \ggmix ~is that it first requires computing the covariance matrix with a computation time of $\mathcal{O}(n^2k)$ followed by a spectral decomposition of this matrix in $\mathcal{O}(n^3)$ time where $k$ is the number of SNP genotypes used to construct the covariance matrix. This computation becomes prohibitive for large cohorts such as the UK Biobank~\citep{allen2012uk} which have collected genetic information on half a million individuals. When the matrix of genotypes used to construct the covariance matrix is low rank, there are additional computational speedups that can be implemented. While this has been developed for the univariate case~\citep{lippert2011fast}, to our knowledge, this has not been explored in the multivariable case. We are currently developing a low rank version of the penalized LMM developed here, which reduces the time complexity from $\mathcal{O}(n^2k)$ to $\mathcal{O}(nk^2)$. \textbf{There is also the issue of how our model scales with an increasing number of covariates ($p$). Due to the coordinate-wise optimization procedure, we expect this to be less of an issue, but still prohibitive for $p>1e5$. The \texttt{biglasso} package~\citep{zeng2017biglasso} uses memory mapping strategies for large $p$, and this is something we are exploring for \texttt{ggmix}.}

\textbf{As was brought up by a reviewer, the simulations and real data analyses presented here contained many more markers used to estimate the kinship than the sample size ($n/k \leq 0.1$). In the single locus association test, Yang el al.~\citep{yang2014advantages} found that proximal contamination was an issue when $n/k \approx 1$. We believe further theoretical study is needed to see if these results can be generalized to the multivariable models being fit here. Once the computational limitations of sample size mentioned above have been addressed, these theoretical results can be supported by simulation studies.}

%There is thus a need to develop newer methodologies that reflect the increasing size and genetic heterogeneity of the large cohort studies being assembled today.



%The LMM-lasso paper mentions that this is possible but does not provide further details on how this can be implemented in a penalized mixed model framework.
There are other applications in which our method could be used as well.
For example, there has been a renewed interest in polygenic risk scores (PRS) which aim to predict complex diseases from genotypes. 
\ggmix ~could be used to build a PRS with the distinct advantage of modeling SNPs jointly, allowing for main effects as well as interactions to be accounted for. 
Based on our results, \ggmix ~has the potential to produce more robust and parsimonious models than the \texttt{lasso} with better predictive accuracy.
%For example, in the most recent Genetic Analysis Workship 20 (GAW20),  the causal modeling group investigated causal
%relationships between DNA methylation (exposure) within some genes
%and the change in high-density lipoproteins $\Delta$HDL (outcome) using Mendelian randomization (MR)~\citep{davey2003mendelian}.
%Penalized regression methods could be used to select SNPs strongly associated with the exposure in order to be used as an instrumental variable (IV).
%However, since GAW20 data consisted of families, two step methods were used which could have resulted in a large number of false positives. \ggmix~is an alternative approach that could be used for selecting the IV while accounting for the familial structure of the data.
Our method is also suitable for fine mapping SNP association signals in genomic regions, where the goal is to pinpoint individual variants most likely to impact the undelying biological mechanisms of disease~\citep{spain2015strategies}.

\begin{comment}
\subsection{Real Data UKBiobank}

Use height as a phenotype as it is known to be related to lots of SNPs with small effects (see paper by Peter Visher, Nature Genetics 2010).


Use 301 SNPs plus those that are in LD. Make sure that overlap is strong with the 300.



\subsection{Extension to Elastic Net}


\subsection{Is there a way to show mathematically, situations where two step is worse than one step?}

\subsection{Other Details}

Since we include a column of ones in our design matrix $\bX$, the first column of the rotation $\bXtilde = \bU^T\bX$ corresponds to the intercept. Therefore, when using \texttt{glmnet} to update $\bbeta$ in Algorithm~\ref{alg:cgd2}, we specify \texttt{intercept = FALSE} and a \texttt{penalty.factor = c(0, rep(1, p))} so that the intercept does not get penalized. Furthermore we specify \texttt{standardize = FALSE}.
\end{comment}










\begin{comment}
\section{General Notes}

\begin{itemize}
\item One way in which spurious associations occur in the presence of
population structure is that SNPs become correlated with each other
when structure is not taken into account~\citep{song2015testing}
\item  For the remainder of the manuscript we therefore focus
on results obtained using the pruned set of SNPs to estimate
kinships (apart for genome-wide analysis in the program Mendel,
which by default always uses the entire set of SNPs that has been
read in)~\citep{eu2014comparison}
\item Most LMM packages (although not Mendel) allow a separation
between the 'estimation of kinships' step and the 'association
testing' step. This is convenient as it allows the user to read in
theoretical or estimated kinships as desired, and to consider using
an alternative package for estimating kinships to the one used for
the actual association testing~\citep{eu2014comparison}
\item Since many groups (including ourselves)
use PLINK [27] to perform initial quality control of genome-wide
association data, we considered programs that could use PLINK
files directly (or with just a few easily-implemented transformation
steps) to be the easiest to use, while those programs that required
more extensive data transformation, creation of additional input
files and/or external estimation of kinships were considered
harder~\citep{eu2014comparison}
\item  investigated a strategy for
analysing longitudinal traits (repeated measures) in a linear mixed
model framework simply by treating each measurement as if it
came from a different individual, and expanding out the genetic
data set accordingly (resulting in an expanded data set containing
many apparent twins, triplets, quadruplets etc., depending on how
many measurements are available for each person).~\citep{eu2014comparison}
\end{itemize}
\end{comment}



\section{Materials and Methods}\label{sec:methods}

%\subsection{Penalized Linear Mixed Models}

%SNP genotypes can be coded as dummy variables with homozygotes being assigned a 0.0, heterozygotes being a 0.5, and opposite homozygotes being a 1.0 under an additive model or, for models involving dominance or recessive effects, with heterozygotes being assigned a 0.0 or 1.0, respectively. For the analyses we describe below, we assumed an additive model.


\subsection{Model Set-up}

Let $i = 1, \ldots, N$ be a grouping index, $j = 1, \ldots, n_i$ the observation index within a group and $N_T = \sum_{i=1}^{N} n_i$ the total number of observations. For each group let \mbox{$\by_i = (y_1, \ldots, y_{n_i})$} be the observed vector of responses or phenotypes, $\bX_i$ an $n_i \times (p + 1)$ design matrix (with the column of 1s for the intercept), $\bb_i$ a group-specific random effect vector of length $n_i$ and \mbox{$\be_i = (\varepsilon_{i1}, \ldots, \varepsilon_{in_i})$} the individual error terms. Denote the stacked vectors $\bY = (\by_i, \ldots, \by_N)^T \in \mathbb{R}^{N_T \times 1}$, $\bb = (\bb_i, \ldots, \bb_N)^T \in \mathbb{R}^{N_T \times 1}$, \mbox{$\be = (\be_i, \ldots, \be_N)^T \in \mathbb{R}^{N_T \times 1}$}, and the stacked matrix \\\mbox{$\bX = (\bX_1^T, \ldots, \bX_N^T) \in \mathbb{R}^{N_T \times (p + 1)}$}. Furthermore, let $\bbeta = (\beta_0,\beta_1, \ldots, \beta_p)^T \in \mathbb{R}^{(p+1) \times 1}$ be a vector of fixed effects regression coefficients corresponding to $\bX$. We consider the following linear mixed model with a single random effect~\citep{pirinen2013efficient}:
\begin{equation}
	\bY = \bX \bbeta + \bb + \be
\end{equation}
where the random effect $\bb$ and the error variance $\be$ are assigned the distributions
\begin{equation}
	\bb \sim \mathcal{N}(0, \eta \sigma^2 \bPhi) \qquad \be \sim \mathcal{N}(0, (1-\eta)\sigma^2 \bI)
\end{equation}
Here, $\bPhi_{N_T \times N_T}$ is a known positive semi-definite and symmetric covariance or kinship matrix calculated from SNPs sampled across the genome,   $\bI_{N_T \times N_T}$ is the identity matrix and parameters $\sigma^2$ and $\eta \in [0,1]$ determine how the variance is divided between $\bb$ and $\be$. Note that $\eta$ is also the narrow-sense heritability ($h^2$), defined as the proportion of phenotypic variance attributable to the additive genetic factors~\citep{manolio2009finding}. The joint density of $\bY$ is therefore multivariate normal:
\begin{equation}
	\bY | (\bbeta, \eta, \sigma^2) \sim \mathcal{N}(\bX \bbeta, \eta \sigma^2 \bPhi + (1-\eta)\sigma^2 \bI) \label{eq:prinen}
\end{equation}

The LMM-Lasso method~\citep{rakitsch2013lasso} considers an alternative but equivalent parameterization given by:
\begin{equation}
	\bY | (\bbeta, \delta, \sigma_g^2) \sim \mathcal{N}(\bX \bbeta, \sigma_g^2(\bPhi + \delta\bI)) \label{eq:lippert}
\end{equation}

where $\delta = \sigma^2_e / \sigma^2_g$, $\sigma^2_g$ is the genetic variance and $\sigma^2_e$ is the residual variance. We instead consider the parameterization in~\eqref{eq:prinen} since maximization is easier over the compact set $\eta \in [0,1]$ than over the unbounded interval $\delta \in [0, \infty)$~\citep{pirinen2013efficient}. We define the complete parameter vector as $\bTheta \coloneqq \left(\bbeta, \eta, \sigma^2 \right)$. The negative log-likelihood for~\eqref{eq:prinen} is given by
\begin{align}
	-\ell(\bTheta) & \propto \frac{N_T}{2}\log(\sigma^2) + \frac{1}{2}\log\left(\det(\bV)\right) + \frac{1}{2\sigma^2} \left(\bY - \bX \bbeta\right)^T \bV^{-1} \left(\bY - \bX \bbeta\right)  \label{eq:LogLike}
\end{align}
where $\bV = \eta \bPhi + (1-\eta) \bI$ and $\det(\bV)$ is the determinant of $\bV$.

Let $\bPhi = \bU \bD \bU^T$ be the eigen (spectral) decomposition of the kinship matrix $\bPhi$, where $\bU_{N_T \times N_T}$ is an orthonormal matrix of eigenvectors (i.e. $\bU \bU^T = \bI$) and $\bD_{N_T \times N_T}$ is a diagonal matrix of eigenvalues $\Lambda_i$. $\bV$ can then be further simplified~\citep{pirinen2013efficient}
\begin{align}
	\bV & = \eta \bPhi + (1-\eta) \bI \nonumber \\
	& = \eta \bU \bD \bU^T + (1-\eta) \bU \bI \bU^T \nonumber \\
	& = \bU \eta  \bD \bU^T + \bU (1-\eta) \bI \bU^T \nonumber \\
	& = \bU \left(\eta  \bD + (1-\eta) \bI\right) \bU^T \nonumber \\
	& = \bU \widetilde{\bD} \bU^T  \label{eq:Vsimplified}
\end{align}
where
\begin{align}
	\widetilde{\bD} & = \eta  \bD + (1-\eta) \bI  \label{eq:LambdaTilde} \\
	& =  \eta \left[ \begin{array}{cccc}
		\Lambda_1 & \hfill & \hfill & \hfill  \\
		\hfill & \Lambda_2 & \hfill & \hfill  \\
		\hfill & \hfill & \ddots &\hfill  \\
		\hfill & \hfill & \hfill & \Lambda_{N_T}  \\
	\end{array} \right] + (1-\eta) \left[ \begin{array}{cccc}
	1 & \hfill & \hfill & \hfill  \\
	\hfill & 1 & \hfill & \hfill  \\
	\hfill & \hfill & \ddots &\hfill  \\
	\hfill & \hfill & \hfill & 1  \\
\end{array} \right]  \nonumber \\
& =   \left[ \begin{array}{cccc}
	1 + \eta (\Lambda_1-1) & \hfill & \hfill & \hfill  \\
	\hfill & 1 + \eta (\Lambda_2-1) & \hfill & \hfill  \\
	\hfill & \hfill & \ddots &\hfill  \\
	\hfill & \hfill & \hfill & 1 + \eta (\Lambda_{N_T}-1)  \\
\end{array} \right]   \nonumber \\
& = \diag\left\lbrace 1 + \eta (\Lambda_1-1), 1 + \eta (\Lambda_2-1), \ldots, 1 + \eta (\Lambda_{N_T}-1) \label{eq:DiagLambda} \right\rbrace
\end{align}
Since~\eqref{eq:LambdaTilde} is a diagonal matrix, its inverse is also a diagonal matrix:
\begin{align}
	\widetilde{\bD}^{-1} & = \diag\left\lbrace \frac{1}{1 + \eta (\Lambda_1-1)}, \frac{1}{1 + \eta (\Lambda_2-1)}, \ldots, \frac{1}{1 + \eta (\Lambda_{N_T}-1)} \label{eq:DiagInvLambda} \right\rbrace
\end{align}


From~\eqref{eq:Vsimplified} and~\eqref{eq:DiagLambda}, $\log(\det(\bV))$ simplifies to
\begin{align}
	\log(\det(\bV)) & =  \log  \left(  \det(\bU) \det\left(\widetilde{\bD}\right) \det(\bU^T)\right)   \nonumber \\
	& =\log\left\lbrace \prod_{i=1}^{N_T}  \left( 1 + \eta (\Lambda_i-1) \right)  \right\rbrace \nonumber \\
	& = \sum_{i=1}^{N_T} \log(1 + \eta (\Lambda_i-1)) \label{eq:LogDetV}
\end{align}
since $\det(\bU) = 1$. It also follows from~\eqref{eq:Vsimplified} that
\begin{align}
	\bV^{-1} & = \left( \bU \widetilde{\bD} \bU^T \right)^{-1} \nonumber \\
	& = \left( \bU^T \right)^{-1}  \left(\widetilde{\bD}\right)^{-1}    \bU^{-1} \nonumber \\
	& = \bU \widetilde{\bD}^{-1} \bU^T \label{eq:Vinv}
\end{align}
since for an orthonormal matrix $\bU^{-1} = \bU^T$. Substituting~\eqref{eq:DiagInvLambda},~\eqref{eq:LogDetV} and~\eqref{eq:Vinv} into~\eqref{eq:LogLike} the negative log-likelihood becomes
\begin{align}
	-\ell(\bTheta) & \propto \frac{N_T}{2}\log(\sigma^2) + \frac{1}{2} \sum_{i=1}^{N_T} \log(1 + \eta (\Lambda_i-1)) + \frac{1}{2\sigma^2} \left(\bY - \bX \bbeta\right)^T \bU \widetilde{\bD}^{-1} \bU^T \left(\bY - \bX \bbeta\right) \label{eq:Likelihood} \\
	& = \frac{N_T}{2}\log(\sigma^2) + \frac{1}{2} \sum_{i=1}^{N_T} \log(1 + \eta (\Lambda_i-1)) + \frac{1}{2\sigma^2} \left(\bU^T\bY - \bU^T\bX \bbeta\right)^T \widetilde{\bD}^{-1} \left(\bU^T\bY - \bU^T\bX \bbeta\right)  \nonumber\\
	& = \frac{N_T}{2}\log(\sigma^2) + \frac{1}{2} \sum_{i=1}^{N_T} \log(1 + \eta (\Lambda_i-1)) + \frac{1}{2\sigma^2} \left(\bYtilde - \bXtilde \bbeta\right)^T \widetilde{\bD}^{-1} \left(\bYtilde - \bXtilde \bbeta\right)  \nonumber\\
	%& = \frac{N_T}{2}\log(\sigma^2) + \frac{1}{2} \sum_{i=1}^{N_T} \log(1 + \eta (\Lambda_i-1)) + \frac{1}{2\sigma^2} \sum_{i=1}^{N_T}\frac{([ \bYtilde - \bXtilde \bbeta]_i )^2}{1 + \eta (\Lambda_i-1)}  \label{eq:LikeFinal}\\
	& = \frac{N_T}{2}\log(\sigma^2) + \frac{1}{2} \sum_{i=1}^{N_T} \log(1 + \eta (\Lambda_i-1)) + \frac{1}{2\sigma^2} \sum_{i=1}^{N_T}\frac{\left(  \Ytilde_i - \sum_{j=0}^{p}\Xtilde_{ij+1}\beta_j \right) ^2}{1 + \eta (\Lambda_i-1)}  \label{eq:LikeFinal}
\end{align}
where $\bYtilde = \bU^T \bY$, $\bXtilde = \bU^T \bX$, $\Ytilde_i$ denotes the $i^{\tm{th}}$ element of $\bYtilde$, $\Xtilde_{ij}$ is the $i,j^{\tm{th}}$ entry of $\bXtilde$ and $\mathbf{1}$ is a column vector of $N_T$ ones.
%and $[\bU^T \mathbf{1}]_{i}$ is the $i^{\tm{th}}$ element of $\bU^T \mathbf{1}$.



%\subsection{Compressed LMM}

%\cite{zhang2010mixed} propose a compressed LMM where substituting n individuals with a smaller number of groups, s (s $\leq$ n), clustered based on the kinship among individuals, e.g., by averaging the SNP data for individuals over the members of each group.


\subsection{Penalized Maximum Likelihood Estimator}
We define the $p+3$  length vector of parameters $\bTheta \coloneqq \left(\Theta_0, \Theta_1, \ldots, \Theta_{p+1}, \Theta_{p+2}, \Theta_{p+3}\right) =  \left(\bbeta, \eta, \sigma^2 \right)$ where $\bbeta \in \mathbb{R}^{p+1}, \eta \in [0,1], \sigma^2 >0$. In what follows, $p+2$ and $p+3$ are the indices in $\bTheta$ for $\eta$ and $\sigma^2$, respectively. In light of our goals to select variables associated with the response in high-dimensional data, we propose to place a constraint on the magnitude of the regression coefficients. This can be achieved by adding a penalty term to the likelihood function~\eqref{eq:LikeFinal}. The penalty term is a necessary constraint because in our applications, the sample size is much smaller than the number of predictors. We define the following objective function:
\begin{equation}
	Q_{\lambda}(\bTheta) = f(\bTheta) + \lambda \sum_{j\neq 0} v_j P_j(\beta_j)
\end{equation}
where $f(\bTheta)\coloneqq-\ell(\bTheta)$ is defined in~\eqref{eq:LikeFinal}, $P_j(\cdot)$ is a penalty term on the fixed regression coefficients $\beta_1, \ldots, \beta_{p+1}$ (we do not penalize the intercept) controlled by the nonnegative regularization parameter $\lambda$, and $v_j$ is the penalty factor for $j$th covariate. These penalty factors serve as a way of allowing parameters to be penalized differently. Note that we do not penalize $\eta$ or $\sigma^2$. An estimate of the regression parameters $\widehat{\bTheta}_{\lambda}$ is obtained by
\begin{equation}
	\widehat{\bTheta}_{\lambda} = \argmin_{\bTheta} Q_{\lambda}(\bTheta) \label{eq:estimator}
\end{equation}
This is the general set-up for our model. In Section~\ref{sec:section3} we provide more specific details on how we solve~\eqref{eq:estimator}. \textbf{We note here that the main difference between the proposed model, and the \texttt{lmmlasso}~\citep{schelldorfer2011estimation}, is that we are limiting ourselves to a single unpenalized random effect. Another key difference is that we rotate the response vector $Y$ and the design matrix $X$ by the eigen vectors of the kinship matrix, resulting in a diagonal covariance matrix which greatly speeds up the computation.} 

\subsection{Computational Algorithm} \label{sec:section3}

%To solve for~\eqref{eq:estimator} we use a block relaxation technique~\citep{de1994block} given by Algorithm~\ref{alg:cgd2}

We use a general purpose block coordinate gradient descent algorithm (CGD)~\citep{tseng2009coordinate} to solve~\eqref{eq:estimator}. At each iteration, we cycle through the coordinates and minimize the objective function with respect to one coordinate only. For continuously differentiable $f(\cdot)$ and convex and block-separable $P(\cdot)$ \mbox{(i.e. $P(\bbeta) = \sum_i P_i (\beta_i)$)}, Tseng and Yun~\cite{tseng2009coordinate} show that the solution generated by the CGD method is a stationary point of $Q_{\lambda}(\cdot)$ if the coordinates are updated in a Gauss-Seidel manner i.e. $Q_{\lambda}(\cdot)$ is minimized with respect to one parameter while holding all others fixed. The CGD algorithm has been successfully applied in fixed effects models (e.g.~\cite{meier2008group},~\cite{friedman2010regularization}) and linear mixed models with an $\ell_1$ penalty~\cite{schelldorfer2011estimation}. In the next section we provide some brief details about Algorithm~\ref{alg:cgd2}. A more thorough treatment of the algorithm is given in Appendix~\ref{ap:cgd}.


\begin{algorithm}[h]
	\SetAlgoLined
	%	\KwResult{Write here the result }
	Set the iteration counter $k \leftarrow 0$, initial values for the parameter vector $\bTheta^{(0)}$ and convergence threshold $\epsilon$\;
	\For{$\lambda \in \left\lbrace \lambda_{max}, \ldots, \lambda_{min} \right\rbrace$}{
		\Repeat{convergence criterion is satisfied: $\norm{\bTheta^{(k+1)} - \bTheta^{(k)}}_2 < \epsilon$}{
			\begin{align*}
				\bbeta^{(k+1)} &\leftarrow \argmin_{\bbeta} Q_{\lambda}\left( \bbeta, \eta^{(k)}, {\sigma^2}^{\,\,(k)}\right) \\
				\eta^{(k+1)} &\leftarrow \argmin_{\eta} Q_{\lambda}\left( \bbeta^{(k+1)}, \eta, {\sigma^2}^{\,\,(k)}\right) \\
				{\sigma^2}^{\,\,(k+1)} &\leftarrow \argmin_{\sigma^2} Q_{\lambda}\left( \bbeta^{(k+1)}, \eta^{(k+1)}, \sigma^2\right)
			\end{align*}

			$k \leftarrow k +1$
		}
	}
	\caption{Block Coordinate Gradient Descent} \label{alg:cgd2}
\end{algorithm}




\subsubsection{Updates for the $\beta$ parameter}
Recall that the part of the objective function that depends on $\bbeta$ has the form
\begin{equation}
	Q_{\lambda}(\bTheta) = \frac{1}{2} \sum_{i=1}^{N_T}w_i\left(  \Ytilde_i - \sum_{j=0}^{p}\Xtilde_{ij+1}\beta_j \right) ^2 + \lambda \sum_{j=1}^{p} v_j \abs{\beta_j} \label{eq:Qlambdalasso2}
\end{equation}
where
\begin{equation}
	w_i \coloneqq \frac{1}{{\sigma^2}\left(1+\eta(\Lambda_i-1)\right)} \label{eq:weights}
\end{equation}

Conditional on $\eta^{(k)}$ and ${\sigma^2}^{\,(k)}$, it can be shown that the solution for $\beta_j$, $j=1, \ldots, p$ is given by
\begin{align}
	\beta_j^{(k+1)} & \gets \frac{\mathcal{S}_{\lambda}\left( \sum_{i=1}^{N_T} w_i \Xtilde_{ij}\left(  \Ytilde_i - \sum_{\ell \neq j}\Xtilde_{i\ell} \beta_\ell^{(k)} \right)\right) }{\sum_{i=1}^{N_T} w_i \Xtilde_{ij}^2} \label{eq:betaUpdateSoft}
\end{align}
where $\mathcal{S}_{\lambda}(x)$ is the soft-thresholding operator
\begin{equation*}
	\mathcal{S}_{\lambda}(x) = \tm{sign}(x)(|x| - \lambda)_+
\end{equation*}
$\textrm{sign}(x)$ is the signum function \[\textrm{sign}(x) = \begin{cases}
		-1 & x<0\\
		0 & x= 0\\
		1 & x>0
	\end{cases}
\] and $(x)_+ = \max(x, 0)$. We provide the full derivation in Appendix~\ref{ap:beta}.

\begin{comment}
\subsection{Updates for the $\beta$ parameter}
Recall that the part of the objective function that depends on $\bbeta$ has the form
\begin{equation}
	Q_{\lambda}(\bTheta) = \frac{1}{2} \sum_{i=1}^{N_T}w_i\left(  \Ytilde_i - \sum_{j=0}^{p}\Xtilde_{ij+1}\beta_j \right) ^2 + \lambda \sum_{j=1}^{p} v_j \abs{\beta_j} \label{eq:Qlambdalasso2}
\end{equation}
where
\begin{equation}
	w_i \coloneqq \frac{1}{{\sigma^2}\left(1+\eta(\Lambda_i-1)\right)} \label{eq:weights}
\end{equation}
However \texttt{glmnet} solves the following problem:
\begin{align}
	\bbeta^{(k+1)} \leftarrow \argmin_{\bbeta}  \frac{1}{2 \sum_{i=1}^{N_T} \widetilde{w}_i^{(k)}  } \sum_{i=1}^{N_T}\widetilde{w}_i^{(k)}\left(  \Ytilde_i - \sum_{j=0}^{p}\Xtilde_{ij+1}\beta_j \right) ^2 + \lambda \sum_{j=1}^{p} v_j  \abs{\beta_j}  \label{eq:LikeFinalBeta}
\end{align}
where
%\begin{equation}
%w_i^{(k)} \coloneqq \frac{1}{{\sigma^2}^{\,(k)}\left(1+\eta^{(k)}(\Lambda_i-1)\right)} \label{eq:weights}
%\end{equation}
%and
\begin{equation}
	\widetilde{w}_i^{(k)} = N_T \cdot \frac{w_i^{(k)}}{\sum_{i=1}^{N_T} w_i^{(k)}}
\end{equation}
Note that $\sum_i \widetilde{w}_i^{(k)} = N_T$. We can simplify~\eqref{eq:LikeFinalBeta} to be:
\begin{align}
	\bbeta^{(k+1)} & \leftarrow \argmin_{\bbeta}  \frac{1}{2 N_T  } \sum_{i=1}^{N_T} N_T \cdot \frac{w_i^{(k)}}{\sum_{i=1}^{N_T} w_i^{(k)}} \left(  \Ytilde_i - \sum_{j=0}^{p}\Xtilde_{ij+1}\beta_j \right) ^2 + \lambda \sum_{j=1}^{p} v_j  \abs{\beta_j} \nonumber \\
	\bbeta^{(k+1)} & \leftarrow \argmin_{\bbeta}  \frac{1}{2 \sum_{i=1}^{N_T} w_i^{(k)}  } \sum_{i=1}^{N_T}  w_i^{(k)} \left(  \Ytilde_i - \sum_{j=0}^{p}\Xtilde_{ij+1}\beta_j \right) ^2 + \lambda \sum_{j=1}^{p} v_j  \abs{\beta_j}  \label{eq:glmnetbeta}
\end{align}
In order to make~\eqref{eq:Qlambdalasso2} to be in the form of~\eqref{eq:glmnetbeta}, we must scale the lambda accordingly:
\begin{align}
	\bbeta^{(k+1)} & \leftarrow \argmin_{\bbeta}  \frac{1}{2  } \sum_{i=1}^{N_T}  w_i^{(k)} \left(  \Ytilde_i - \sum_{j=0}^{p}\Xtilde_{ij+1}\beta_j \right) ^2 + \frac{\lambda}{\sum_{i=1}^{N_T} w_i^{(k)}} \sum_{j=1}^{p}  v_j  \abs{\beta_j}  \label{eq:glmnetbeta2}
\end{align}


Conditional on $\eta^{(k)}$ and ${\sigma^2}^{\,(k)}$, it can be shown that the solution for $\bbeta$ is a weighted lasso problem with observation weights given by~\eqref{eq:weights}.

The full derivation is given in Section~\ref{subsec:l1penalty}. Therefore, $\bbeta^{(k+1)}$ can be efficiently solved using the \texttt{glmnet} algorithm~\citep{friedman2010regularization}. Note that the rescaling of the weights to sum to $N_T$ is what is being done in \texttt{glmnet}.
\end{comment}




\subsubsection{Updates for the $\eta$ paramter}
Given $\bbeta^{(k+1)}$ and ${\sigma^2}^{\,(k)}$, solving for $\eta^{(k+1)}$ becomes a univariate optimization problem:
\begin{equation}
	\eta^{(k+1)} \leftarrow \argmin_{\eta}  \frac{1}{2} \sum_{i=1}^{N_T} \log(1 + \eta (\Lambda_i-1)) + \frac{1}{2{\sigma^2}^{\,(k)}} \sum_{i=1}^{N_T}\frac{\left(  \Ytilde_i - \sum_{j=0}^{p}\Xtilde_{ij+1}\beta_j^{(k+1)} \right) ^2}{1 + \eta (\Lambda_i-1)}
\end{equation}
We use a bound constrained optimization algorithm~\citep{byrd1995limited} implemented in the \texttt{optim} function in \texttt{R} and set the lower and upper bounds to be 0.01 and 0.99, respectively.



\subsubsection{Updates for the $\sigma^2$ parameter}

Conditional on $\bbeta^{(k+1)}$ and $\eta^{(k+1)}$, ${\sigma^2}^{\,(k+1)}$ can be solved for using the following equation:
\begin{equation}
	{\sigma^2}^{\,(k+1)} \leftarrow \argmin_{\sigma^2}  \frac{N_T}{2}\log(\sigma^2) + \frac{1}{2\sigma^2} \sum_{i=1}^{N_T}\frac{\left(  \Ytilde_i - \sum_{j=0}^{p}\Xtilde_{ij+1}\beta_j \right) ^2}{1 + \eta (\Lambda_i-1)} \label{eq:sigma}
\end{equation}

There exists an analytic solution for~\eqref{eq:sigma} given by:
\begin{align}
	%\frac{\partial}{\partial \sigma^2} Q_{\lambda}(\bTheta) &= \frac{N_T}{2\sigma^2}- \frac{1}{2\sigma^4} \sum_{i=1}^{N_T}\frac{\left(  \Ytilde_i - \sum_{j=0}^{p}\Xtilde_{ij+1}\beta_j^{(k+1)} \right) ^2}{1 + \eta^{(k+1)} (\Lambda_i-1)} = 0 \nonumber \\
	{\sigma^2}^{\,(k+1)} & \gets \frac{1}{N_T}\sum_{i=1}^{N_T}\frac{\left(  \Ytilde_i - \sum_{j=0}^{p}\Xtilde_{ij+1}\beta_j^{(k+1)} \right) ^2}{1 + \eta^{(k+1)} (\Lambda_i-1)} \label{eq:sigmahat2}
\end{align}


\subsubsection{Regularization path}
In this section we describe how determine the sequence of tuning parameters $\lambda$ at which to fit the model. Recall that our objective function has the form
\begin{equation}
	Q_{\lambda}(\bTheta) = \frac{N_T}{2}\log(\sigma^2) + \frac{1}{2} \sum_{i=1}^{N_T} \log(1 + \eta (\Lambda_i-1)) + \frac{1}{2} \sum_{i=1}^{N_T}w_i\left(  \Ytilde_i - \sum_{j=0}^{p}\Xtilde_{ij+1}\beta_j \right) ^2 + \lambda \sum_{j=1}^{p}  v_j  \abs{\beta_j} \label{eq:Qlambdalasso}
\end{equation}
The Karush-Kuhn-Tucker (KKT) optimality conditions for~\eqref{eq:Qlambdalasso} are given by:
\begin{equation}
	\begin{aligned}
		\frac{\partial}{\partial \beta_1, \ldots, \beta_p} Q_{\lambda}(\bTheta) &= \mathbf{0}_p   \\
		\frac{\partial}{\partial \beta_0} Q_{\lambda}(\bTheta) &= 0 \\
		\frac{\partial}{\partial \eta} Q_{\lambda}(\bTheta) &= 0  \\
		\frac{\partial}{\partial \sigma^2} Q_{\lambda}(\bTheta) &= 0
	\end{aligned} \label{eq:kktgrad}
\end{equation}

\begin{comment}
\eqref{eq:kktbeta} is equivalent to
\begin{align}
%\frac{1}{\sigma^2} \sum_{i=1}^{N_T}\frac{\sum_{j=1}^{p}\Xtilde_{ij+1}\left(  \Ytilde_i - \beta_0 [\bU^T \mathbf{1}]_{i} -  \sum_{j=1}^{p}\Xtilde_{ij+1}\beta_j \right)}{1 + \eta (\Lambda_i-1)} & = \lambda \gamma \nonumber\\
\bXtilde^T_{-1}\bW \left(\bYtilde - \bXtilde\bbeta\right) & = \lambda \gamma  \label{eq:kktbeta2}
\end{align}
\begin{equation}
\gamma_j \in \begin{cases}
\tm{sign}(\hat{\beta}_j) & \tm{if} \quad \hat{\beta}_j \neq 0 \\
[-1,1] & \tm{if}\quad \hat{\beta}_j = 0
\end{cases}, \qquad \tm{for }j=1, \ldots, p  \label{eq:kktsubgradient}
\end{equation}
where $\bW \coloneqq \bDtilde^{-1}$ given by~\eqref{eq:DiagInvLambda}, $\bXtilde^T_{-1}$ is $\bXtilde^T$ with the first column removed, and $\gamma \in \mathbb{R}^p$ is the subgradient function of the $\ell_1$ norm evaluated at $(\hat{\beta}_1, \ldots, \hat{\beta}_p)$.
\eqref{eq:kktbeta0} is equivalent to
\begin{align}
\bU^T \mathbf{1} \bW \left(\bYtilde - \bU^T \mathbf{1} \beta_0\right) = 0 \label{eq:kktbeta02}
\end{align}
\eqref{eq:kktsigma} is equivalent to
\begin{align}
{\sigma^2} - \frac{1}{N_T}\left(\bYtilde - \bXtilde \bbeta\right)^T \bW \left(\bYtilde - \bXtilde \bbeta\right) = 0 \label{eq:kktsigma2}
\end{align}
Therefore $\widehat{\bTheta}$ is a solution in~\eqref{eq:estimator} if and only if $\widehat{\bTheta}$ satisfies~\eqref{eq:kktbeta2},~\eqref{eq:kktsubgradient},~\eqref{eq:kktbeta02},~\eqref{eq:kkteta} and~\eqref{eq:kktsigma2} for some $\gamma$.
\end{comment}

The equations in~\eqref{eq:kktgrad} are equivalent to
\begin{equation}
	\begin{aligned}
		\sum_{i=1}^{N_T}w_i \Xtilde_{i1}\left(  \Ytilde_i - \sum_{j=0}^{p}\Xtilde_{ij+1}\beta_j \right)  = 0\\
		\frac{1}{v_j} \sum_{i=1}^{N_T}w_i \Xtilde_{ij}\left(  \Ytilde_i - \sum_{j=0}^{p}\Xtilde_{ij+1}\beta_j \right) =  \lambda \gamma_j, \\
		\gamma_j \in \begin{cases}
			\tm{sign}(\hat{\beta}_j) & \tm{if} \quad \hat{\beta}_j \neq 0 \\
			[-1,1] & \tm{if}\quad \hat{\beta}_j = 0
		\end{cases}, \qquad \tm{for }j=1, \ldots, p   \\
		\frac{1}{2} \sum_{i=1}^{N_T} \frac{\Lambda_i - 1}{1 + \eta(\Lambda_i - 1)} \left(1- \frac{\left(  \Ytilde_i - \sum_{j=0}^{p}\Xtilde_{ij+1}\beta_j \right) ^2}{\sigma^2 (1+\eta(\Lambda_i-1))}  \right) = 0  \\
		{\sigma^2} - \frac{1}{N_T}\sum_{i=1}^{N_T}\frac{\left(  \Ytilde_i - \sum_{j=0}^{p}\Xtilde_{ij+1}\beta_j \right) ^2}{1 + \eta (\Lambda_i-1)} = 0
	\end{aligned}\label{eq:kktsolved}
\end{equation}
where $w_i$ is given by~\eqref{eq:weights}, $\bXtilde^T_{-1}$ is $\bXtilde^T$ with the first column removed, $\bXtilde^T_1$ is the first column of $\bXtilde^T$, and $\boldsymbol{\gamma} \in \mathbb{R}^p$ is the subgradient function of the $\ell_1$ norm evaluated at $(\hat{\beta}_1, \ldots, \hat{\beta}_p)$. Therefore $\widehat{\bTheta}$ is a solution in~\eqref{eq:estimator} if and only if $\widehat{\bTheta}$ satisfies~\eqref{eq:kktsolved} for some $\gamma$. %The solution path is given by an inductive verification of the KKT conditions~\citep{osborne2000lasso}, i.e., if~\eqref{eq:kktsolved} holds as $\lambda$ decreases, then we know we have a solution.
%we find the solutinon for the other paramters such that the KKT conditions are verified.
%page 17 of sswith learning
We can determine a decreasing sequence of tuning parameters by starting at a maximal value for $\lambda = \lambda_{max}$ for which $\hat{\beta}_j = 0$ for $j=1, \ldots, p$. In this case, the KKT conditions in~\eqref{eq:kktsolved} are equivalent to
\begin{equation}
	\begin{aligned}
		%\abs{\bXtilde^T_{j}\bW \bYtilde}  \leq \lambda, \quad \forall j=1, \ldots,p  \\
		\frac{1}{v_j} \sum_{i=1}^{N_T}\abs{w_i \Xtilde_{ij}\left(  \Ytilde_i - \Xtilde_{i1}\beta_0 \right)} \leq \lambda, \quad \forall j=1, \ldots,p \\
		%\gamma_j \in \begin{cases}
		%\tm{sign}(\hat{\beta}_j) & \tm{if} \quad \hat{\beta}_j \neq 0 \\
		%[-1,1] & \tm{if}\quad \hat{\beta}_j = 0
		%\end{cases}, \qquad \tm{for }j=1, \ldots, p   \\
		%\beta_0 = \frac{\bXtilde^T_1 \bW \bYtilde}{\bXtilde^T_1 \bW\bXtilde_1 } =  \frac{\bXtilde^T_1 \bDtilde^{-1} \bYtilde}{\bXtilde^T_1 \bDtilde^{-1}	\bXtilde_1 }  \\
		\beta_0 = \frac{\sum_{i=1}^{N_T}w_i \Xtilde_{i1}\Ytilde_i }{\sum_{i=1}^{N_T}w_i \Xtilde_{i1}^2}\\
		%\frac{1}{2} \sum_{i=1}^{N_T} \frac{\Lambda_i - 1}{1 + \eta(\Lambda_i - 1)} \left(1- \frac{(  \Ytilde_i - \Xtilde_{i1}\beta_0 ) ^2}{\sigma^2 (1 + \eta (\Lambda_i-1))}  \right) = 0  \\
		\frac{1}{2} \sum_{i=1}^{N_T} \frac{\Lambda_i - 1}{1 + \eta(\Lambda_i - 1)} \left(1- \frac{\left(  \Ytilde_i - \Xtilde_{i1}\beta_0 \right) ^2}{\sigma^2(1+\eta(\Lambda_i-1))}  \right) = 0\\
		%{\sigma^2} = \frac{1}{N_T}\left(\bYtilde - \bXtilde_1 \beta_0 \right)^T \widetilde{\bD}^{-1} \left(\bYtilde - \bXtilde_1 \beta_0\right) \\
		{\sigma^2} = \frac{1}{N_T}\sum_{i=1}^{N_T}\frac{\left(  \Ytilde_i - \Xtilde_{i1}\beta_0 \right) ^2}{1 + \eta (\Lambda_i-1)}
	\end{aligned}\label{eq:kktsolvedmax}
\end{equation}
We can solve the KKT system of equations in~\eqref{eq:kktsolvedmax} (with a numerical solution for $\eta$) in order to have an explicit form of the stationary point $\widehat{\bTheta}_0 = \left\lbrace \hat{\beta}_0, \mathbf{0}_p, \hat{\eta}, \widehat{\sigma}^2 \right\rbrace$. Once we have $\widehat{\bTheta}_0$, we can solve for the smallest value of $\lambda$ such that the entire vector ($\hat{\beta}_1, \ldots, \hat{\beta}_p$) is 0:
%\begin{equation}
%\lambda_{max} = \max_j \abs{\bXtilde^T_{j}\bW \bYtilde}, \quad j=1, \ldots, p
%\end{equation}
\begin{equation}
	\lambda_{max} = \max_j \left\lbrace \abs{ \frac{1}{v_j} \sum_{i=1}^{N_T}\hat{w_i} \Xtilde_{ij}\left(  \Ytilde_i - \Xtilde_{i1}\hat{\beta}_0 \right)}\right\rbrace , \quad j=1, \ldots, p
\end{equation}
Following Friedman et al.~\cite{friedman2010regularization}, we choose $\tau\lambda_{max}$ to be the smallest value of tuning parameters $\lambda_{min}$, and construct a
sequence of $K$ values decreasing from $\lambda_{max}$ to $\lambda_{min}$ on the log scale. The defaults are set to $K = 100$, $\tau = 0.01$ if $n < p $ and $\tau = 0.001$ if $n \geq p $.


\subsubsection{Warm Starts}
The way in which we have derived the sequence of tuning parameters using the KKT conditions, allows us to implement warm starts. That is, the solution $\widehat{\bTheta}$ for $\lambda_k$ is used as the initial value $\bTheta^{(0)}$ for $\lambda_{k+1}$. This strategy leads to computational speedups and has been implemented in the \texttt{ggmix} R package.


\subsubsection{Prediction of the random effects}
We use an empirical Bayes approach (e.g.~\cite{wakefield2013bayesian}) to predict the random effects $\bb$. Let the maximum a posteriori (MAP) estimate be defined as
\begin{equation}
	\widehat{\bb} = \argmax_{\bb} f(\bb |  \bY, \bbeta, \eta, \sigma^2)  \label{eq:MAP}
\end{equation}
where, by using Bayes rule, $f(\bb |  \bY, \bbeta, \eta, \sigma^2)$ can be expressed as
\begin{align}
	f(\bb |  \bY, \bbeta, \eta, \sigma^2) & = \frac{f(\bY | \bb,  \bbeta, \eta, \sigma^2)  \pi(\bb | \eta, \sigma^2)}{f(\bY |  \bbeta, \eta, \sigma^2)} \nonumber \\
	& \propto f(\bY | \bb,  \bbeta, \eta, \sigma^2)  \pi(\bb | \eta, \sigma^2) \nonumber\\
	& \propto \exp \left\lbrace - \frac{1}{2 \sigma^2} (\bY - \bX \bbeta - \bb)^T  (\bY - \bX \bbeta - \bb) - \frac{1}{2\eta \sigma^2}\bb^T \bPhi^{-1}\bb   \right \rbrace \nonumber\\
	& = \exp \left\lbrace - \frac{1}{2 \sigma^2} \left[  (\bY - \bX \bbeta - \bb)^T (\bY - \bX \bbeta - \bb) + \frac{1}{\eta }\bb^T \bPhi^{-1}\bb \right]    \right \rbrace \label{eq:MAP2}
\end{align}
Solving for~\eqref{eq:MAP} is equivalent to minimizing the exponent in~\eqref{eq:MAP2}:
\begin{align}
	\widehat{\bb} = \argmin_{\bb} \left\lbrace (\bY - \bX \bbeta - \bb)^T  (\bY - \bX \bbeta - \bb) + \frac{1}{\eta }\bb^T \bPhi^{-1}\bb  \right\rbrace \label{eq:MAP3}
\end{align}
Taking the derivative of~\eqref{eq:MAP3} with respect to $\bb$ and setting it to 0 we get:
\begin{align}
	0 & = -2  (\bY - \bX \bbeta - \bb) + \frac{2}{\eta} \bPhi^{-1}\bb \nonumber \\
	& = - (\bY - \bX \bbeta ) + \bb +  \left( \frac{1}{\eta}\bPhi^{-1}\right) \bb  \nonumber\\
	(\bY - \bX \bbeta ) & = \left(\bI_{N_T \times N_T} +  \frac{1}{\eta}\bPhi^{-1}\right) \bb  \nonumber\\
	\widehat{\bb} & = \left( \bI_{N_T \times N_T} +  \frac{1}{\widehat{\eta}}\bPhi^{-1}\right)^{-1}  (\bY - \bX \widehat{\bbeta} ) \nonumber \\
	& = \left( \bI_{N_T \times N_T} +  \frac{1}{\widehat{\eta}} \bU \bD^{-1} \bU^T\right)^{-1}  (\bY - \bX \widehat{\bbeta} )
\end{align}
where $(\widehat{\bbeta}, \widehat{\eta})$ are the estimates obtained from Algorithm~\ref{alg:cgd2}.
% \nonumber \\
%& = \left(\bU \bDtilde^{-1} \bU^T + \frac{1}{\widehat{\eta}}\bU \bD^{-1} %\bU^T\right)^{-1} \bU \bDtilde^{-1} \bU^T (\bY - \bX \widehat{\bbeta} ) \nonumber \\
%& = \left(\bU \left[\bDtilde^{-1} + \frac{1}{\widehat{\eta}} \bD^{-1} \right] %\bU^T \right)^{-1} \bU \bDtilde^{-1} (\bYtilde - \bXtilde \widehat{\bbeta} ) \nonumber \\
%& = \bU \left[\bDtilde^{-1} + \frac{1}{\widehat{\eta}} \bD^{-1} \right]^{-1} \bU^T \bU \bDtilde^{-1} (\bYtilde - \bXtilde \widehat{\bbeta} ) \nonumber
%where $\bV^{-1}$ is given by~\eqref{eq:Vinv}, and $(\widehat{\bbeta}, \widehat{\eta})$ are the estimates obtained from Algorithm~\ref{alg:cgd2}.
%Using standard results from linear mixed models (e.g.~\cite{wakefield2013bayesian}), the random effects are predicted by
%\begin{equation}
%\widehat{\bb} = \widehat{\eta} \bPhi \bU \bDtilde^{-1} \bU^T (\bY - \bX \widehat{\bbeta})
%\end{equation}



\subsubsection{Phenotype prediction} \label{phenoprediction}

Here we describe the method used for predicting the unobserved phenotype $\bY^\star$ in a set of individuals with predictor set $\bX^\star$ that were not used in the model training e.g. a testing set. Let $q$ denote the number of observations in the testing set and $N-q$ the number of observations in the training set. We assume that a \texttt{ggmix} model has been fit on a set of training individuals with observed phenotype $\bY$ and predictor set $\bX$.  We further assume that $\bY$ and $\bY^\star$ are jointly multivariate Normal:

\begin{equation}
\left[ \begin{matrix*}[c]
\bY^\star \\
\bY
\end{matrix*}\right] \sim \mathcal{N} \left( \left[\begin{matrix*}[c]
\bmu_{1_{(q\times 1)}} \\
\bmu_{2_{(N-q)\times 1}}
\end{matrix*}\right], \left[ \begin{matrix}
\bSigma_{11_{(q\times q)}} & \bSigma_{12_{q\times (N-q)}} \\
\bSigma_{21_{(N-q)\times q}} & \bSigma_{22_{(N-q)\times (N-q)}}  \\
\end{matrix}   \right]  \right)
\end{equation}


Then, from standard multivariate Normal theory, the conditional distribution $\bY^\star | \bY, \eta, \sigma^2, \bbeta,\bX, \bX^\star$ is $\mathcal{N}(\bmu^\star, \bSigma^\star)$ where

\begin{align}
\bmu^\star &= \bmu_1 + \bSigma_{12} \bSigma_{22}^{-1} (\bY - \mu_2) \\
\bSigma^\star & = \bSigma_{11} - \bSigma_{12} \bSigma_{22}^{-1} \bSigma_{21}
\end{align}

The phenotype prediction is thus given by:

\begin{align}
\bmu^\star_{q \times 1} & = \bX^\star \bbeta + \frac{1}{\sigma^2} \bSigma_{12} \bV^{-1} (\bY - \bX \bbeta)\\
& = \bX^\star \bbeta + \frac{1}{\sigma^2} \bSigma_{12} \bU \bDtilde^{-1} \bU^T (\bY - \bX \bbeta)\\
& = \bX^\star \bbeta + \frac{1}{\sigma^2} \bSigma_{12} \bU \bDtilde^{-1} (\bYtilde - \bXtilde \bbeta ) \\
& = \bX^\star \bbeta + \frac{1}{\sigma^2} \eta \sigma^2 \bPhi^\star \bU \bDtilde^{-1} (\bYtilde - \bXtilde \bbeta ) \\
& = \bX^\star \bbeta +  \eta  \bPhi^\star \bU \bDtilde^{-1} (\bYtilde - \bXtilde \bbeta )
\end{align}

where $\bPhi^\star$ is the $q \times (N-q)$ covariance matrix between the testing and training individuals.



\subsubsection{Choice of the optimal tuning parameter}

In order to choose the optimal value of the tuning parameter $\lambda$, we use the generalized information criterion~\citep{nishii1984asymptotic} (GIC):
\begin{equation}
	GIC_{\lambda} = -2 \ell(\widehat{\bbeta}, \widehat{\sigma}^2, \widehat{\eta}) + a_n \cdot \widehat{df}_{\lambda}
\end{equation}
where $\widehat{df}_{\lambda}$ is the number of non-zero elements in $\widehat{\bbeta}_{\lambda}$~\citep{zou2007degrees} plus two (representing the variance parameters $\eta$ and $\sigma^2$). Several authors have used this criterion for variable selection in mixed models with $a_n = \log N_T$~\citep{bondell2010joint,schelldorfer2011estimation}, which corresponds to the BIC. We instead choose the high-dimensional BIC~\citep{fan2013tuning} given by $a_n = \log(\log(N_T)) * \log(p)$. This is the default choice in our \texttt{ggmix} R package, though the interface is flexible to allow the user to select their choice of $a_n$.












\newpage


%\section*{Acknowledgments}



%\section*{Reproducibility}


\section*{Availability of data and material}
%The data sets supporting the results of this article are available in the repositories:
\begin{enumerate}
  \item The UK Biobank data is available upon successful project application.
	\item The GAW20 data is freely available upon request from \url{https://www.gaworkshop.org/data-sets}.
	\item Mouse cross data is available from GitHub at \url{https://github.com/sahirbhatnagar/ggmix/blob/pgen/RealData/mice.RData}. 
	\item The entire simulation study is reproducible. Source code available at \url{https://github.com/sahirbhatnagar/ggmix/tree/pgen/simulation}. This includes scripts for \ggmix, ~\texttt{lasso} and \texttt{twostep} methods. 
	\item The R package \ggmix ~is freely available from GitHub at \url{https://github.com/greenwoodlab/ggmix}.
	\item A website describing how to use the package is available at \url{https://sahirbhatnagar.com/ggmix/}.
\end{enumerate}

\section*{Competing interests}
The authors declare that they have no competing interests.

\section*{Author's contributions}
SRB, KO, YY and CMTG conceived the idea. SRB developped the algorithms, software and simulation study. TL completed the real data analysis. ES and JCLO provided data and interpretations. SRB, TL and CMTG wrote a draft of the manuscript then all authors edited, read and approved the final manuscript.

\section*{Acknowledgements}
SRB was supported by the Ludmer Centre for Neuroinformatics and Mental Health and the Canadian Institutes for Health Research PJT 148620. This research was enabled in part by support provided by Calcul Québec (www.calculquebec.ca) and Compute Canada (www.computecanada.ca). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.

\section*{Supporting Information}
%\subsection*{Additional file 1 -- Supplemental Methods and Simulation Results}
Contains the following sections:
\begin{itemize}
	\item[A] \textbf{Block Coordinate Descent Algorithm} - a detailed description of the algorithm used to fit our \ggmix ~model.
	 \item[B] \textbf{Additional Real Data Analysis Results} - supporting information for the GAW20 and UK Biobank analyses
	\item[C] \textbf{\ggmix ~Package Showcase} - a vignette describing how to use our \ggmix ~\texttt{R} package
\end{itemize}

%\bibliographystyle{unsrt}
%\bibliography{GEbib}

%\bibliographystyle{unsrt}
\bibliographystyle{vancouver}
\bibliography{ggmixbib}

\newpage

\appendix
\counterwithin{figure}{section}

\section{Block Coordinate Descent Algorithm} \label{ap:cgd}

We use a general purpose block coordinate descent algorithm (CGD)~\citep{tseng2009coordinate} to solve~\eqref{eq:estimator}. At each iteration, the algorithm approximates the negative log-likelihood $f(\cdot)$ in $Q_{\lambda}(\cdot)$ by a strictly convex quadratic function and then applies block coordinate decent to generate a decent direction followed by an inexact line search along this direction~\citep{tseng2009coordinate}. For continuously differentiable $f(\cdot)$ and convex and block-separable $P(\cdot)$ \mbox{(i.e. $P(\bbeta) = \sum_i P_i (\beta_i)$)},~\cite{tseng2009coordinate} show that the solution generated by the CGD method is a stationary point of $Q_{\lambda}(\cdot)$ if the coordinates are updated in a Gauss-Seidel manner i.e. $Q_{\lambda}(\cdot)$ is minimized with respect to one parameter while holding all others fixed. The CGD algorithm can thus be run in parallel and therefore suited for large $p$ settings. It has been successfully applied in fixed effects models (e.g.~\cite{meier2008group},~\cite{friedman2010regularization}) and~\cite{schelldorfer2011estimation} for mixed models with an $\ell_1$ penalty. Following Tseng and Yun~\cite{tseng2009coordinate}, the CGD algorithm is given by Algorithm~\ref{alg:cgd}.

\begin{algorithm}[htbp]
	\SetAlgoLined
	%	\KwResult{Write here the result }
	Set the iteration counter $k \leftarrow 0$ and choose initial values for the parameter vector $\bTheta^{(0)}$\;
	\Repeat{convergence criterion is satisfied}{
		Approximate the Hessian $\nabla^2 f(\bTheta^{(k)})$ by a symmetric matix $H^{(k)}$:
		\begin{equation}
			H^{(k)} = \diag \left[ \min \left\lbrace \max \left\lbrace \left[ \nabla^2 f(\bTheta^{(k)})\right] _{jj}, c_{min} \right\rbrace c_{max} \right\rbrace\right]_{j = 1, \ldots, p} \label{eq:Hk}
		\end{equation}
		\For{$ j =1, \ldots, p$}{

			Solve the descent direction $d^{(k)} \coloneqq d_{H^{(k)}}(\Theta_{j}^{(k)})$ \;

			\If{$\Theta_j^{(k)} \in \left\lbrace \beta_1, \ldots, \beta_p \right\rbrace$}{
				\begin{equation}
					d_{H^{(k)}}(\Theta_{j}^{(k)}) \leftarrow \argmin_{d} \left\lbrace \nabla f(\Theta_{j}^{(k)}) d + \frac{1}{2} d^2 H^{(k)}_{j j} + \lambda P(\Theta_{j}^{(k)} + d) \right\rbrace \label{eq:descentdirection}
				\end{equation}
			}
			%{ \If{$\Theta_j^{(k)} \in \left\lbrace \eta \right\rbrace$}{
			%	\begin{equation}
			%		d_{H^{(k)}}(\Theta_{j}^{(k)}) \leftarrow - \nabla f(\Theta_{j}^{(k)}) / H_{jj}^{(k)}
			%	\end{equation}
			%}
	}
	Choose a stepsize\;
	\begin{equation*}
		\alpha_j^{(k)} \leftarrow \tm{line search given by the Armijo rule}
	\end{equation*}
	Update\;
	\begin{equation*}
		\widehat{\Theta}_j^{(k+1)} \leftarrow \widehat{\Theta}_j^{(k)} + \alpha_j^{(k)}d^{(k)}
	\end{equation*}
Update\;
\begin{equation}
	\widehat\eta^{(k+1)} \leftarrow \argmin_{\eta}  \frac{1}{2} \sum_{i=1}^{N_T} \log(1 + \eta (\Lambda_i-1)) + \frac{1}{2{\sigma^2}^{\,(k)}} \sum_{i=1}^{N_T}\frac{\left(  \Ytilde_i - \sum_{j=0}^{p}\Xtilde_{ij+1}\beta_j^{(k+1)} \right) ^2}{1 + \eta (\Lambda_i-1)}
\end{equation}
Update\;
\begin{equation}
	%\widehat{\sigma^2}^{\,\,(k+1)} \leftarrow \frac{1}{N_T}\sum_{i=1}^{N_T}\frac{([ \bYtilde - \bXtilde \widehat{\bbeta}^{(k+1)}]_i )^2}{1 + \widehat{\eta}^{(k+1)} (\Lambda_i-1)}
	\widehat{\sigma^2}^{\,\,(k+1)} \gets \frac{1}{N_T}\sum_{i=1}^{N_T}\frac{\left(  \Ytilde_i - \sum_{j=0}^{p}\Xtilde_{ij+1}\beta_j^{(k+1)} \right) ^2}{1 + \eta^{(k+1)} (\Lambda_i-1)}
\end{equation}
$k \leftarrow k +1$
}
\caption{Coordinate Gradient Descent Algorithm to solve~\eqref{eq:estimator}} \label{alg:cgd}
\end{algorithm}

\begin{comment}
We note that conditional on $\widehat{\bbeta}$ and $\widehat{\eta}$, there exists an analytic solution for $\widehat{\sigma^2}$:
\begin{align}
	\frac{\partial}{\partial \sigma^2} f(\bTheta) &= \frac{N_T}{2\sigma^2}- \frac{1}{2\sigma^4} \sum_{i=1}^{N_T}\frac{([ \bYtilde - \bXtilde \bbeta]_i )^2}{1 + \eta (\Lambda_i-1)} = 0 \nonumber \\
	\widehat{\sigma^2} & = \frac{1}{N_T}\sum_{i=1}^{N_T}\frac{([ \bYtilde - \bXtilde \widehat{\bbeta}]_i )^2}{1 + \widehat{\eta} (\Lambda_i-1)} \label{eq:sigmahat}
\end{align}
\end{comment}

The Armijo rule is defined as follows~\citep{tseng2009coordinate}:
\begin{tcolorbox}
	Choose $\alpha_{init}^{(k)}>0$ and let $\alpha^{(k)}$ be the largest element of $\left\lbrace \alpha_{init}^k \delta^r \right\rbrace_{r = 0,1,2,\ldots} $ satisfying
	\begin{equation}
		Q_{\lambda}(\Theta_j^{(k)} + \alpha^{(k)} d^{(k)}) \leq Q_{\lambda} (\Theta_j^{(k)}) + \alpha^{(k)}\varrho \Delta^{(k)}
	\end{equation}
	where $0 < \delta <1$, $0 < \varrho <1$, $0 \leq \gamma < 1$ and
	\begin{equation}
		\Delta^{(k)} \coloneqq \nabla f(\Theta_j^{(k)})d^{(k)} + \gamma (d^{(k)})^2 H^{(k)}_{jj} + \lambda P(\Theta_j^{(k)} + d^{(k)}) - \lambda P(\Theta^{(k)})
	\end{equation}
\end{tcolorbox}
Common choices for the constants are $\delta=0.1$, $\varrho=0.001$, $\gamma = 0$, $\alpha_{init}^{(k)} = 1$ for all $k$~\citep{schelldorfer2011estimation}.

Below we detail the specifics of Algorithm~\ref{alg:cgd} for the $\ell_1$ penalty.

\subsection{$\ell_1$ penalty}\label{subsec:l1penalty}

The objective function is given by
\begin{equation}
	Q_{\lambda}(\bTheta) = f(\bTheta) + \lambda |\bbeta|
\end{equation}


\subsubsection{Descent Direction}
For simplicity, we remove the iteration counter $(k)$ from the derivation below.\\ For \mbox{$\Theta_j^{(k)} \in \left\lbrace \beta_1, \ldots, \beta_p \right\rbrace$}, let
\begin{equation}
	d_{H}(\Theta_{j}) = \argmin_{d} G(d)  \label{eq:argminGd}
\end{equation}
where
\[ G(d) =  \nabla f(\Theta_{j}) d + \frac{1}{2} d^2 H_{j j} + \lambda |\Theta_{j} + d| \]
Since $G(d)$ is not differentiable at $-\Theta_j$, we calculate the subdifferential $\partial G(d)$ and search for $d$ with $0 \in \partial G(d)$:
\begin{equation}
	\partial G(d) = \nabla f(\Theta_{j}) + d H_{j j} + \lambda u   \label{eq:subdiff}
\end{equation}
where
\begin{equation}
	u = \begin{cases}
		1 & \tm{if\quad}d > -\Theta_j \\
		-1 & \tm{if\quad} d < -\Theta_j\\
		[-1,1] & \tm{if\quad} d = \Theta_j
	\end{cases}
\end{equation}
We consider each of the three cases in~\eqref{eq:subdiff} below
\begin{enumerate}
	\item $d > -\Theta_j$
	\begin{align}
		\partial G(d) & = \nabla f(\Theta_{j}) + d H_{j j} + \lambda = 0 \nonumber \\
		d & = \frac{-(\nabla f(\Theta_{j}) + \lambda)}{H_{j j}}  \nonumber
	\end{align}
	Since $\lambda>0$ and $H_{jj}>0$, we have
	\begin{equation*}
		\frac{-(\nabla f(\Theta_{j}) - \lambda)}{H_{j j}} > \frac{-(\nabla f(\Theta_{j}) + \lambda)}{H_{j j}} = d \overset{\tm{def}}{>} -\Theta_j
	\end{equation*}
	The solution can be written compactly as
	\begin{equation*}
		d = \tm{mid}\left\lbrace \frac{-(\nabla f(\Theta_{j}) - \lambda)}{H_{j j}}, -\Theta_j ,\frac{-(\nabla f(\Theta_{j}) + \lambda)}{H_{j j}} \right\rbrace
	\end{equation*}
	where $\tm{mid}\left\lbrace a,b,c \right\rbrace$ denotes the median (mid-point) of $a,b,c$~\citep{tseng2009coordinate}.
	\item $d < -\Theta_j$
	\begin{align}
		\partial G(d) & = \nabla f(\Theta_{j}) + d H_{j j} - \lambda = 0 \nonumber \\
		d & = \frac{-(\nabla f(\Theta_{j}) - \lambda)}{H_{j j}}  \nonumber
	\end{align}
	Since $\lambda>0$ and $H_{jj}>0$, we have
	\begin{equation*}
		\frac{-(\nabla f(\Theta_{j}) + \lambda)}{H_{j j}} < \frac{-(\nabla f(\Theta_{j}) - \lambda)}{H_{j j}} = d \overset{\tm{def}}{<} -\Theta_j
	\end{equation*}
	Again, the solution can be written compactly as
	\begin{equation*}
		d = \tm{mid}\left\lbrace \frac{-(\nabla f(\Theta_{j}) - \lambda)}{H_{j j}}, -\Theta_j ,\frac{-(\nabla f(\Theta_{j}) + \lambda)}{H_{j j}} \right\rbrace
	\end{equation*}

	\item $d_j = -\Theta_j$\\
	There exists $u \in [-1,1]$ such that
	\begin{align*}
		\partial G(d) & = \nabla f(\Theta_{j}) + d H_{j j} + \lambda u = 0 \nonumber \\
		d & = \frac{-(\nabla f(\Theta_{j}) + \lambda u)}{H_{j j}}  \nonumber
	\end{align*}
	For $-1 \leq u \leq 1$, $\lambda>0$ and $H_{jj}>0$ we have
	\begin{equation*}
		\frac{-(\nabla f(\Theta_{j}) + \lambda)}{H_{j j}} \leq  d \overset{\tm{def}}{=} -\Theta_j \leq \frac{-(\nabla f(\Theta_{j}) - \lambda)}{H_{j j}}
	\end{equation*}
	The solution can again be written compactly as
	\begin{equation*}
		d = \tm{mid}\left\lbrace \frac{-(\nabla f(\Theta_{j}) - \lambda)}{H_{j j}}, -\Theta_j ,\frac{-(\nabla f(\Theta_{j}) + \lambda)}{H_{j j}} \right\rbrace
	\end{equation*}

\end{enumerate}
We see all three cases lead to the same solution for~\eqref{eq:argminGd}. Therefore the descent direction for $\Theta_j^{(k)} \in \left\lbrace \beta_1, \ldots, \beta_p \right\rbrace$ for the $\ell_1$ penalty is given by
\begin{equation}
	d = \tm{mid}\left\lbrace \frac{-(\nabla f(\beta_{j}) - \lambda)}{H_{j j}}, -\beta_j ,\frac{-(\nabla f(\beta_{j}) + \lambda)}{H_{j j}} \right\rbrace  \label{eq:d}
\end{equation}

\subsubsection{Solution for the $\beta$ parameter} \label{ap:beta}
If the Hessian $\nabla^2f(\bTheta^{(k)}) >0$ then $H^{(k)}$ defined in~\eqref{eq:Hk} is equal to $\nabla^2f(\bTheta^{(k)})$. Using $\alpha_{init} = 1$, the largest element of $\left\lbrace \alpha_{init}^{(k)} \delta^r \right\rbrace_{r = 0, 1, 2, \ldots}$ satisfying the Armijo Rule inequality is reached for $\alpha^{(k)} = \alpha_{init}^{(k)}\delta^0 = 1$. The Armijo rule update for the $\bbeta$ parameter is then given by
\begin{equation}
	\beta_j^{(k+1)} \leftarrow \beta_j^{(k)} + d^{(k)}, \qquad j=1, \ldots, p \label{eq:betaupdate}
\end{equation}
Substituting the descent direction given by~\eqref{eq:d} into~\eqref{eq:betaupdate} we get
\begin{equation}
	\beta_j^{(k+1)} = \tm{mid}\left\lbrace \beta_j^{(k)}+ \frac{-(\nabla f(\beta_j^{(k)}) - \lambda)}{H_{j j}}, 0,\beta_j^{(k)}+ \frac{-(\nabla f(\beta_j^{(k)}) + \lambda)}{H_{j j}}  \right\rbrace \label{eq:betaMidpoint}
\end{equation}
We can further simplify this expression. Let %Let $\bXtilde_{-j}$ and $\bbeta^{(k)}_{-j}$ correspond to $\bXtilde$ and $\bbeta^{(k)}$ without the $j^{\tm{th}}$ variable, respectively. Furthermore, let

\begin{equation}
	w_i \coloneqq \frac{1}{\sigma^2\left(1+\eta(\Lambda_i-1)\right)}
\end{equation}.


Re-write the part depending on $\bbeta$ of the negative log-likelihood in~\eqref{eq:LikeFinal} as
\begin{align}
	%\frac{1}{2\sigma^2} \sum_{i=1}^{N_T}\frac{\left(  \Ytilde_i - \sum_{j=1}^{p}\Xtilde_{ij}\beta_j^{(k)} \right) ^2}{1 + \eta (\Lambda_i-1)}
	g(\bbeta^{(k)}) & = \frac{1}{2} \sum_{i=1}^{N_T} w_i\left(  \Ytilde_i - \sum_{\ell \neq j}\Xtilde_{i\ell} \beta_\ell^{(k)} - \Xtilde_{ij}\beta_j^{(k)} \right) ^2
\end{align}
The gradient and Hessian are given by
\begin{align}
	\nabla f(\beta_j^{(k)}) \coloneqq \frac{\partial}{\partial \beta_j^{(k)}}g(\bbeta^{(k)}) & = - \sum_{i=1}^{N_T} w_i \Xtilde_{ij}\left(  \Ytilde_i - \sum_{\ell \neq j}\Xtilde_{i\ell} \beta_\ell^{(k)} - \Xtilde_{ij}\beta_j^{(k)} \right)  \label{eq:grad}\\
	H_{jj} \coloneqq \frac{\partial^2}{\partial {\beta_j^{(k)}}^2}g(\bbeta^{(k)}) & = \sum_{i=1}^{N_T} w_i \Xtilde_{ij}^2  \label{eq:hessian}
\end{align}
Substituting~\eqref{eq:grad} and~\eqref{eq:hessian} into $\beta_j^{(k)}+ \frac{-(\nabla f(\beta_j^{(k)}) - \lambda)}{H_{jj}}$ %~\eqref{eq:betaMidpoint} we get
\begin{align}
	& \beta_j^{(k)}+ \frac{  \sum_{i=1}^{N_T} w_i \Xtilde_{ij}\left(  \Ytilde_i - \sum_{\ell \neq j}\Xtilde_{i\ell} \beta_\ell^{(k)} - \Xtilde_{ij}\beta_j^{(k)} \right)  + \lambda }{\sum_{i=1}^{N_T} w_i \Xtilde_{ij}^2} \nonumber \\
	& = \beta_j^{(k)}+ \frac{ \sum_{i=1}^{N_T} w_i \Xtilde_{ij}\left(  \Ytilde_i - \sum_{\ell \neq j}\Xtilde_{i\ell} \beta_\ell^{(k)} \right) + \lambda}{\sum_{i=1}^{N_T} w_i \Xtilde_{ij}^2} - \frac{\sum_{i=1}^{N_T} w_i \Xtilde_{ij}^2\beta_j^{(k)}  }{\sum_{i=1}^{N_T} w_i \Xtilde_{ij}^2} \nonumber \\
	& =  \frac{  \sum_{i=1}^{N_T} w_i \Xtilde_{ij}\left(  \Ytilde_i - \sum_{\ell \neq j}\Xtilde_{i\ell} \beta_\ell^{(k)} \right) + \lambda}{\sum_{i=1}^{N_T} w_i \Xtilde_{ij}^2} \label{eq:midleft}
\end{align}
Similarly, substituting~\eqref{eq:grad} and~\eqref{eq:hessian} in $\beta_j^{(k)}+ \frac{-(\nabla f(\beta_j^{(k)}) + \lambda)}{H_{jj}}$ we get
\begin{align}
	\frac{  \sum_{i=1}^{N_T} w_i \Xtilde_{ij}\left(  \Ytilde_i - \sum_{\ell \neq j}\Xtilde_{i\ell} \beta_\ell^{(k)} \right) - \lambda}{\sum_{i=1}^{N_T} w_i \Xtilde_{ij}^2} \label{eq:midright}
\end{align}
Finally, substituting~\eqref{eq:midleft} and~\eqref{eq:midright} into~\eqref{eq:betaMidpoint} we get
\begin{align}
	\beta_j^{(k+1)} & = \tm{mid}\left\lbrace \frac{  \sum_{i=1}^{N_T} w_i \Xtilde_{ij}\left(  \Ytilde_i - \sum_{\ell \neq j}\Xtilde_{i\ell} \beta_\ell^{(k)} \right) - \lambda}{\sum_{i=1}^{N_T} w_i \Xtilde_{ij}^2}, 0,\frac{  \sum_{i=1}^{N_T} w_i \Xtilde_{ij}\left(  \Ytilde_i - \sum_{\ell \neq j}\Xtilde_{i\ell} \beta_\ell^{(k)} \right) + \lambda}{\sum_{i=1}^{N_T} w_i \Xtilde_{ij}^2} \right\rbrace \nonumber \\
	& = \frac{\mathcal{S}_{\lambda}\left( \sum_{i=1}^{N_T} w_i \Xtilde_{ij}\left(  \Ytilde_i - \sum_{\ell \neq j}\Xtilde_{i\ell} \beta_\ell^{(k)} \right)\right) }{\sum_{i=1}^{N_T} w_i \Xtilde_{ij}^2} \label{eq:betaUpdateSoft2}
\end{align}

Where $\mathcal{S}_{\lambda}(x)$ is the soft-thresholding operator
\begin{equation*}
	\mathcal{S}_{\lambda}(x) = \tm{sign}(x)(|x| - \lambda)_+
\end{equation*}
$\textrm{sign}(x)$ is the signum function
\begin{equation*}
	\textrm{sign}(x) = \begin{cases}
		-1 & x<0\\
		0 & x= 0\\
		1 & x>0
	\end{cases}
\end{equation*}
and $(x)_+ = \max(x, 0)$.

%We note that the parameter update for $\beta_j$ given by~\eqref{eq:betaUpdateSoft} takes the same form as the weighted updates of the \texttt{glmnet} algorithm~\citep{friedman2010regularization} (Section 2.4, equation (10)) with $\alpha=1$.


\FloatBarrier

\newpage

\section{Additional Real Data Analysis Results} \label{ap:rda}

\subsection{Distribution of SNPs used in UK Biobank analysis}

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=1\linewidth]{figure/UKB-chromosome-distribution-1} 

}

\caption[Distribution of SNPs used in UK Biobank analysis by chromosome and whether or not the SNP was imputed]{Distribution of SNPs used in UK Biobank analysis by chromosome and whether or not the SNP was imputed.}\label{fig:UKB-chromosome-distribution}
\end{figure}


\end{knitrout}


\newpage

\subsection{LD structure among the markers in the GAW20 and the mouse dataset}\label{ap:ldplots}

%In Table~\ref{tab:print-sim-table-for-n-equal-to-k}, we present additional simulation results for the situation when the sample size ($n=1000$) is equal to the number of SNPs used to estimate the kinship matrix ($k=1000$).  

We illustrate the LD structure among the markers in the GAW20 dataset and the mouse dataset separately in Figures~\ref{fig:gaw20r2} and~\ref{fig:miceR2}, respectively. In Figure~\ref{fig:gaw20r2}, we show the pairwise $r^2$ for 655 SNPs within a 1Mb-window around the causal SNP rs9661059 (indicated) that we focused on. The dotplot above the heatmap denotes $r^2$ between each SNP and the causal SNP. It is clear that although strong correlation does exist between some SNPs, none of these nearby SNPs is correlated with the causal SNP. The only dot denoting an $r^2=1$ represents the causal SNP itself.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\linewidth]{figure/gaw20R2}
	\caption{LD structure among the markers in the GAW20 dataset}
	\label{fig:gaw20r2}
\end{figure}

In Figure~\ref{fig:miceR2}, we show the pairwise $r^2$ for all microsatellite markers in the mouse dataset. It is clear that many markers are considerably strongly correlated with each other, as we expected.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\linewidth]{figure/miceR2}
	\caption{LD structure among the markers in the mouse dataset}
	\label{fig:miceR2}
\end{figure}



 

 
 
%\subsection{Null Model $(c = 0)$}



















%%%%%%%%%%%%%%%%%%%%%% NOT USED%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%\subsection{1\% of SNPs are Causal $(c = 0.01)$}





\FloatBarrier




















%%%%%%%%%%%%%%%%%%%%%%%%%%% NOT USED %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\FloatBarrier

\newpage


\section{\ggmix ~Package Showcase} \label{ap:showcase}

In this section we briefly introduce the freely available and open source \ggmix ~package in \texttt{R}. More comprehensive documentation is available at \url{https://sahirbhatnagar.com/ggmix}. Note that this entire section is reproducible; the code and text are combined in an \texttt{.Rnw}\footnote[1]{scripts available at \url{https://github.com/sahirbhatnagar/ggmix/tree/pgen/manuscript}} file and compiled using \texttt{knitr}~\citep{xie2015dynamic}.

\subsection{Installation}

The package can be installed from \href{https://github.com/sahirbhatnagar/ggmix}{GitHub} via


\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{install.packages}\hlstd{(}\hlstr{"pacman"}\hlstd{)}
\hlstd{pacman}\hlopt{::}\hlkwd{p_load_gh}\hlstd{(}\hlstr{'sahirbhatnagar/ggmix'}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}

To showcase the main functions in \ggmix, ~we will use the simulated data which ships with the package and can be loaded via:

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{## library(ggmix)}
\hlkwd{data}\hlstd{(}\hlstr{"admixed"}\hlstd{)}
\hlkwd{names}\hlstd{(admixed)}
\end{alltt}
\begin{verbatim}
##  [1] "ytrain"         "ytune"          "ytest"          "xtrain"        
##  [5] "xtune"          "xtest"          "xtrain_lasso"   "xtune_lasso"   
##  [9] "xtest_lasso"    "Xkinship"       "kin_train"      "kin_tune_train"
## [13] "kin_test_train" "mu_train"       "causal"         "beta"          
## [17] "not_causal"     "kinship"        "coancestry"     "PC"            
## [21] "subpops"
\end{verbatim}
\end{kframe}
\end{knitrout}

For details on how this data was simulated, see \texttt{help(admixed)}.

There are three basic inputs that \ggmix ~needs:
\begin{enumerate}
	\item $Y$: a continuous response variable
\item $X$: a matrix of covariates of dimension $N \times p$ where $N$ is the sample size and $p$ is the number of covariates
\item $\boldsymbol{\Phi}$: a kinship matrix
\end{enumerate}

We can visualize the kinship matrix in the \texttt{admixed} data using the \texttt{popkin} package:

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# need to install the package if you don't have it}
\hlcom{# pacman::p_load_gh('StoreyLab/popkin')}
\hlstd{popkin}\hlopt{::}\hlkwd{plot_popkin}\hlstd{(admixed}\hlopt{$}\hlstd{kin)}
\end{alltt}


{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in popkin::plot\_popkin(admixed\$kin): Every element of list "{}kinship"{} is NULL!}}\end{kframe}
\end{knitrout}

\subsection{Fit the linear mixed model with Lasso Penalty}

We will use the most basic call to the main function of this package, which is called \texttt{ggmix}. This function will by default fit a $L_1$ penalized linear mixed model (LMM) for 100 distinct values of the tuning parameter $\lambda$. It will choose its own sequence:

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{fit} \hlkwb{<-} \hlkwd{ggmix}\hlstd{(}\hlkwc{x} \hlstd{= admixed}\hlopt{$}\hlstd{xtrain,}
                 \hlkwc{y} \hlstd{= admixed}\hlopt{$}\hlstd{ytrain,}
                         \hlkwc{kinship} \hlstd{= admixed}\hlopt{$}\hlstd{kin_train)}
\hlkwd{names}\hlstd{(fit)}
\end{alltt}
\begin{verbatim}
##  [1] "result"       "ggmix_object" "n_design"     "p_design"     "lambda"      
##  [6] "coef"         "b0"           "beta"         "df"           "eta"         
## [11] "sigma2"       "nlambda"      "cov_names"    "call"
\end{verbatim}
\begin{alltt}
\hlkwd{class}\hlstd{(fit)}
\end{alltt}
\begin{verbatim}
## [1] "lassofullrank" "ggmix_fit"
\end{verbatim}
\end{kframe}
\end{knitrout}

We can see the solution path for each variable by calling the \texttt{plot} method for objects of class \texttt{ggmix\_fit}:

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{plot}\hlstd{(fit)}
\end{alltt}
\end{kframe}

{\centering \includegraphics[width=1\linewidth]{figure/ggmix-solution-path-1} 

}



\end{knitrout}

We can also get the coefficients for given value(s) of lambda using the \texttt{coef} method for objects of class \texttt{ggmix\_fit}:

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# only the first 5 coefficients printed here for brevity}
\hlkwd{coef}\hlstd{(fit,} \hlkwc{s} \hlstd{=} \hlkwd{c}\hlstd{(}\hlnum{0.1}\hlstd{,}\hlnum{0.02}\hlstd{))[}\hlnum{1}\hlopt{:}\hlnum{5}\hlstd{, ]}
\end{alltt}
\begin{verbatim}
## 5 x 2 Matrix of class "dgeMatrix"
##                       1            2
## (Intercept) -0.03715135  0.247105426
## X23          0.00000000  0.098030248
## X36          0.00000000 -0.013022250
## X38          0.00000000  0.005378361
## X40          0.00000000  0.004028934
\end{verbatim}
\end{kframe}
\end{knitrout}
Here, \texttt{s} specifies the value(s) of $\lambda$ at which the extraction is made. The function uses linear interpolation to make predictions for values of \texttt{s} that do not coincide with the lambda sequence used in the fitting algorithm.

We can also get predictions ($X\widehat{\boldsymbol{\beta}}$) using the \texttt{predict} method for objects of class \texttt{ggmix\_fit}:

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# need to provide x to the predict function}
\hlcom{# predict for the first 5 subjects}
\hlkwd{predict}\hlstd{(fit,} \hlkwc{s} \hlstd{=} \hlkwd{c}\hlstd{(}\hlnum{0.1}\hlstd{,}\hlnum{0.02}\hlstd{),} \hlkwc{newx} \hlstd{= admixed}\hlopt{$}\hlstd{xtest[}\hlnum{1}\hlopt{:}\hlnum{5}\hlstd{,])}
\end{alltt}
\begin{verbatim}
##                1           2
## id26  2.30208546  2.45597763
## id39  0.87334032  1.62931898
## id45 -0.12296837 -0.06075786
## id52 -0.03715135 -0.97519671
## id53 -0.21046107 -0.23151040
\end{verbatim}
\end{kframe}
\end{knitrout}


\subsection{Find the Optimal Value of the Tuning Parameter}

We use the Generalized Information Criterion (GIC) to select the optimal value for $\lambda$. The default is $a_n = log(log(n)) * log(p)$ which corresponds to a high-dimensional BIC (HDBIC):

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# pass the fitted object from ggmix to the gic function:}
\hlstd{hdbic} \hlkwb{<-} \hlkwd{gic}\hlstd{(fit)}
\hlkwd{class}\hlstd{(hdbic)}
\end{alltt}
\begin{verbatim}
## [1] "ggmix_gic"     "lassofullrank" "ggmix_fit"
\end{verbatim}
\begin{alltt}
\hlcom{# we can also fit the BIC by specifying the an argument}
\hlstd{bicfit} \hlkwb{<-} \hlkwd{gic}\hlstd{(fit,} \hlkwc{an} \hlstd{=} \hlkwd{log}\hlstd{(}\hlkwd{length}\hlstd{(admixed}\hlopt{$}\hlstd{ytrain)))}
\end{alltt}
\end{kframe}
\end{knitrout}

We can plot the HDBIC values against $\log(\lambda)$ using the \texttt{plot} method for objects of class \texttt{ggmix\_gic}:

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{plot}\hlstd{(hdbic)}
\end{alltt}
\end{kframe}

{\centering \includegraphics[width=1\linewidth]{figure/ggmix-hdbic-1} 

}



\end{knitrout}

The optimal value for $\lambda$ according to the HDBIC, i.e., the $\lambda$ that leads to the minium HDBIC is:

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{hdbic[[}\hlstr{"lambda.min"}\hlstd{]]}
\end{alltt}
\begin{verbatim}
## [1] 0.09862269
\end{verbatim}
\end{kframe}
\end{knitrout}


We can also plot the BIC results:

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{plot}\hlstd{(bicfit,} \hlkwc{ylab} \hlstd{=} \hlstr{"BIC"}\hlstd{)}
\end{alltt}
\end{kframe}

{\centering \includegraphics[width=1\linewidth]{figure/ggmix-bic-1} 

}


\begin{kframe}\begin{alltt}
\hlstd{bicfit[[}\hlstr{"lambda.min"}\hlstd{]]}
\end{alltt}
\begin{verbatim}
## [1] 0.07460445
\end{verbatim}
\end{kframe}
\end{knitrout}


\subsection{Get Coefficients Corresponding to Optimal Model}

We can use the object outputted by the \texttt{gic} function to extract the coefficients corresponding to the selected model using the \texttt{coef} method for objects of class \texttt{ggmix\_gic}:

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{coef}\hlstd{(hdbic)[}\hlnum{1}\hlopt{:}\hlnum{5}\hlstd{, ,} \hlkwc{drop} \hlstd{=} \hlnum{FALSE}\hlstd{]}
\end{alltt}
\begin{verbatim}
## 5 x 1 sparse Matrix of class "dgCMatrix"
##                       1
## (Intercept) -0.03660806
## X23          .         
## X36          .         
## X38          .         
## X40          .
\end{verbatim}
\end{kframe}
\end{knitrout}

We can also extract just the nonzero coefficients which also provide the estimated variance components $\eta$ and $\sigma^2$:

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{coef}\hlstd{(hdbic,} \hlkwc{type} \hlstd{=} \hlstr{"nonzero"}\hlstd{)}
\end{alltt}
\begin{verbatim}
##                       1
## (Intercept) -0.03660806
## X302        -0.17607392
## X524         1.34951500
## X538        -0.72052613
## eta          0.99000000
## sigma2       1.60476289
\end{verbatim}
\end{kframe}
\end{knitrout}

We can also make predictions from the \texttt{hdbic} object, which by default will use the model corresponding to the optimal tuning parameter:

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{predict}\hlstd{(hdbic,} \hlkwc{newx} \hlstd{= admixed}\hlopt{$}\hlstd{xtest[}\hlnum{1}\hlopt{:}\hlnum{5}\hlstd{,])}
\end{alltt}
\begin{verbatim}
##                1
## id26  2.31027410
## id39  0.86922183
## id45 -0.12814532
## id52 -0.03660806
## id53 -0.21268198
\end{verbatim}
\end{kframe}
\end{knitrout}


\subsection{Extracting Random Effects}

The user can compute the random effects using the provided \texttt{ranef} method for objects of class \texttt{ggmix\_gic}. This command will compute the estimated random effects for each subject using the parameters of the selected model:

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{ranef}\hlstd{(hdbic)[}\hlnum{1}\hlopt{:}\hlnum{5}\hlstd{]}
\end{alltt}
\begin{verbatim}
## [1] -2.4889655  1.1834200 -0.5641832 -0.9310334 -0.3458703
\end{verbatim}
\end{kframe}
\end{knitrout}



\subsection{Diagnostic Plots}

We can also plot some standard diagnotic plots such as the observed vs. predicted response, QQ-plots of the residuals and random effects and the Tukey-Anscombe plot. These can be plotted using the \texttt{plot} method on a \texttt{ggmix\_gic} object as shown below.


\subsubsection{Observed vs. Predicted Response}

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{plot}\hlstd{(hdbic,} \hlkwc{type} \hlstd{=} \hlstr{"predicted"}\hlstd{,} \hlkwc{newx} \hlstd{= admixed}\hlopt{$}\hlstd{xtrain,} \hlkwc{newy} \hlstd{= admixed}\hlopt{$}\hlstd{ytrain)}
\end{alltt}
\end{kframe}

{\centering \includegraphics[width=1\linewidth]{figure/ggmix-obs-pred-1} 

}



\end{knitrout}


\subsubsection{QQ-plots for Residuals and Random Effects}

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{plot}\hlstd{(hdbic,} \hlkwc{type} \hlstd{=} \hlstr{"QQranef"}\hlstd{,} \hlkwc{newx} \hlstd{= admixed}\hlopt{$}\hlstd{xtrain,} \hlkwc{newy} \hlstd{= admixed}\hlopt{$}\hlstd{ytrain)}
\end{alltt}
\end{kframe}

{\centering \includegraphics[width=1\linewidth]{figure/ggmix-qqplot-1} 

}


\begin{kframe}\begin{alltt}
\hlkwd{plot}\hlstd{(hdbic,} \hlkwc{type} \hlstd{=} \hlstr{"QQresid"}\hlstd{,} \hlkwc{newx} \hlstd{= admixed}\hlopt{$}\hlstd{xtrain,} \hlkwc{newy} \hlstd{= admixed}\hlopt{$}\hlstd{ytrain)}
\end{alltt}
\end{kframe}

{\centering \includegraphics[width=1\linewidth]{figure/ggmix-qqplot-2} 

}



\end{knitrout}


\subsubsection{Tukey-Anscombe Plot}

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{plot}\hlstd{(hdbic,} \hlkwc{type} \hlstd{=} \hlstr{"Tukey"}\hlstd{,} \hlkwc{newx} \hlstd{= admixed}\hlopt{$}\hlstd{xtrain,} \hlkwc{newy} \hlstd{= admixed}\hlopt{$}\hlstd{ytrain)}
\end{alltt}
\end{kframe}

{\centering \includegraphics[width=1\linewidth]{figure/ggmix-tukey-1} 

}



\end{knitrout}






\end{document}

\section{Low rank similarity matrix}

Let $\mb{K} \in \mathbb{R}^{N_T\times k}$ be the matrix containing the $k$ SNPs used to compute the factored kinship matrix $\bPhi$ given by
\begin{equation}
	\bPhi = \mb{K}\mb{K}^T \label{eq:factored}
\end{equation}

%If we let $\mb{U} \mb{\widetilde{S}} \mb{V}^T$ be the singular value decomposition (SVD) of $\mb{K}$. Then
%\begin{align*}
%\bPhi &= \left(\mb{U} \mb{\widetilde{S}} \mb{V}^T\right)\left(\mb{U} \mb{\widetilde{S}} \mb{V}^T\right)^T \\
%& = \mb{U}\mb{\widetilde{S}}\mb{V}^T\mb{V}\mb{\widetilde{S}}\mb{U}^T\\
%& = \mb{U} \mb{\widetilde{S}}\mb{\widetilde{S}}\mb{U}^T \\
%& = \mb{U} \mb{S} \mb{U}^T
%\end{align*}
%where $\mb{S}_{ii} = \mb{\widetilde{S}}_{ii}\mb{\widetilde{S}}_{ii}$. Therefore, $\mb{U}$ consists of the eigenvectors of $\bPhi$ and the eigenvalues of $\bPhi$ are given by %$\mb{\widetilde{S}}_{ii}^2$ which are the square of the eigenvalues of the SNP matrix $\mb{K} \in \mathbb{R}^{n\times p}$.
%Note that the eigenvectors of $\bPhi$ are equal to the singular vectors of $\mb{K}$, and the eigenvalues of $\bPhi$ are equal to the square of the singular values of %$\mb{K}$~\citep{berrar2003practical}.

Furthermore, let $\mb{K} = \mb{U} \bLambda \mb{V}^T$ be the singular value decomposition (SVD) of $\mb{K}$. Plugging this into~\eqref{eq:factored} we get
\begin{align}
	\bPhi &= \left(\mb{U} \bLambda \mb{V}^T\right)\left(\mb{U} \bLambda \mb{V}^T\right)^T \nonumber\\
	& = \mb{U}\bLambda\mb{V}^T\mb{V}\bLambda\mb{U}^T \nonumber\\
	& = \mb{U} \bLambda\bLambda\mb{U}^T \nonumber\\
	& = \mb{U} \bSigma \mb{U}^T, \label{eq:svdphi}
\end{align}
Therefore, the eigenvectors of $\bPhi$ are equal to the singular vectors of $\mb{K}$ (denoted by $\bU$), and the eigenvalues of $\bPhi$ (denoted by the diagonal matrix $\bSigma$) are equal to the square of the singular values of $\mb{K}$~\citep{berrar2003practical}. This allows us to bypass the explicit computation of the kinship matrix by directly applying SVD on the SNP matrix $\bW$. ~\cite{lippert2011fast} noted that the computational time for fitting the LMM can be reduced if the matrix $\mb{K}$ is not full rank, i.e., when $k < N_T$. This is due to the fact that the matrix $\bD_{N_T \times N_T}$ contains $k$ non-zero eigenvalues followed by $N_T-k$ zeros on the diagonal. Let $\bU \equiv \left[\bU_1 \,\, \bU_2\right]$, where $\bU_1 \in \mathbb{R}^{N_T\times k}$ and $\bU_2 \in \mathbb{R}^{N_T \times (n-k)}$ are  the matrices of singular vectors corresponding to the $k$ non-zero and $N_T-k$ zero eigenvalues, respectively. Then~\eqref{eq:svdphi} can be written as
\begin{equation}
	\bPhi = \bU_1 \bSigma \bU_1^T
\end{equation}
%Following~\cite{lippert2011fast}, we show that the log-likelihood~\eqref{eq:Likelihood} can be expressed in terms of $\bU_1$ only, foregoing the need to compute $\bU_2$.
We now try to simplify the log-likelihood~\eqref{eq:Likelihood}. Since there are $N_T-k$ zero eigenvalues, the second term in~\eqref{eq:Likelihood} reduces to
\begin{equation}
	\frac{1}{2} \left(  \sum_{i=1}^{k} \log(1 + \eta (\Sigma_i-1)) + (N_T-k) \log(1-\eta)\right) \label{eq:term2}
\end{equation}
where $\Sigma_i = \Lambda_i^2$, and $\Lambda_i$ is the $i^{\tm{th}}$ singular value of $\bW$. Let $a \equiv (\bY - \bX \bbeta)$. The third term in~\eqref{eq:Likelihood} can be written as
\begin{align}
	\frac{1}{2\sigma^2} a^T \left[ \eta \bPhi + (1-\eta)\bI_n \right] ^{-1} a &= \frac{1}{2\sigma^2} a^T \left[ \eta \bU_1 \bSigma_1 \bU_1^T + (1-\eta)\bI_n \right] ^{-1} a  \nonumber \\
	& = \frac{1}{2\sigma^2} a^T \left[ \bC \bB \bC^T + \mb{A} \right] ^{-1} a \nonumber
\end{align}
where
\begin{align*}
	\mb{A} & = (1-\eta) \bI_n \\
	\mb{B} & = \bSigma_1 \\
	\mb{C} & = \sqrt{\eta} \bU_1 \\
	\mb{C}^T & = \sqrt{\eta} \bU_1^T
\end{align*}
Assuming $\bC \bB \bC^T + \mb{A}$ is non-singular, the inverse of $\left[ \bC \bB \bC^T + \mb{A} \right]$ is given explicitly by the Woodbury formula~\citep{golub2012matrix}
\begin{align}
	\left(\bA + \bC \bB \bC^T\right)^{-1} & = \bA^{-1} - \bA^{-1} \bC \left(\bB^{-1} + \bC^T\bA^{-1} \bC\right)^{-1}\bC^T \bA^{-1} \label{eq:woodbury}
\end{align}
Substituting the values for $\bA, \bB$ and $\bC$ into~\eqref{eq:woodbury} we get
\begin{align}
	\left(\bA + \bC \bB \bC^T\right)^{-1} & = \frac{1}{1-\eta}\bI_{N_T} - \frac{\sqrt{\eta}}{1-\eta}\bI_{N_T}\bU_1 \left(\bSigma_1^{-1} + \frac{\eta}{1-\eta}\bU_1^T \bI_{N_T} \bU\right)^{-1}\frac{\sqrt{\eta}}{1-\eta}\bU_1^T \bI_{N_T} \nonumber \\
	& = \frac{1}{1-\eta} \left[ \bI_{N_T} - \frac{\eta}{1-\eta}\bU_1 \left(\bSigma_1^{-1} + \frac{\eta}{1-\eta}\bI_{k} \right)^{-1}\bU_1^T \right] \nonumber \\
	& = \frac{1}{1-\eta} \left[ \bI_{N_T} - \frac{\eta}{1-\eta}\bU_1 \left(\frac{\eta}{1-\eta} \left(\frac{1-\eta}{\eta}\bSigma_1^{-1} + \bI_{k}\right) \right)^{-1}\bU_1^T \right] \nonumber \\
	& = \frac{1}{1-\eta} \left[ \bI_{N_T} - \bU_1 \left(\frac{1-\eta}{\eta}\bSigma_1^{-1} + \bI_{k}\right) ^{-1}\bU_1^T \right] \label{eq:term3}
\end{align}


where we have used the following identities: $\bI_k = \bU_1^T\bU_1$, $\bI_{N_T -k} = \bU_2^T\bU_2$.
%, and
%\begin{align}
%\bI_{N_T} & = \bU \bU^T  \nonumber\\
%& =\left[\bU_1 \,\, \bU_2\right] \left[\bU_1 \,\, \bU_2\right] ^T \nonumber\\
%& = \bU_1 \bU_1^T + \bU_2 \bU_2^T \label{eq:In} \nonumber
%\end{align}

Substituting~\eqref{eq:term2} and~\eqref{eq:term3} in~\eqref{eq:Likelihood} we obtain
\begin{align}
	\begin{split}
		-\ell(\bTheta) & \propto \frac{N_T}{2}\log(\sigma^2) + \frac{1}{2} \left(  \sum_{i=1}^{k} \log(1 + \eta (\Sigma_i-1)) + (N_T-k) \log(1-\eta)\right) + \\
		&\frac{1}{2} \left\lbrace \left(\bY - \bX\bbeta \right)^T  \left[\frac{1}{\sigma^2(1-\eta)}\left(  \bI_{N_T} - \bU_1 \left(\frac{1-\eta}{\eta}\bSigma_1^{-1} + \bI_{k}\right) ^{-1}\bU_1^T \right)  \right] \left(\bY - \bX\bbeta \right)  \right\rbrace
	\end{split} \label{eq:loglikrowrank}
\end{align}

\section{Group Lasso with Low-rank Similarity Matrix}
This section focuses on the part of the log-likelihood~\eqref{eq:loglikrowrank} that depends on $\bbeta$.

\subsection{Model}
Only the third term of the log-likelihood~\eqref{eq:loglikrowrank} depends on $\bbeta$:
\begin{equation}
	\frac{1}{2} \left\lbrace \left(\bY - \bX\bbeta \right)^T  \left[\frac{1}{\sigma^2(1-\eta)}\left(  \bI_{N_T} - \bU_1 \left(\frac{1-\eta}{\eta}\bSigma_1^{-1} + \bI_{k}\right) ^{-1}\bU_1^T \right)  \right] \left(\bY - \bX\bbeta \right)  \right\rbrace \label{eq:likeW}
\end{equation}
Equation~\eqref{eq:likeW} can be written more generally as
\[
L(\bbeta\mid\bD)=\frac{1}{2}\left[\bY-\widehat{\bY}\right]^{\top}\mathbf{W}\left[\bY-\widehat{\bY}\right]
\]
where $\widehat{\bY}=\sum_{j=1}^{p}\beta_{j}X_{j}$, $\bD$ is the working data $\lbrace \bY, \bX \rbrace$, and $\bW$ is an $N_T \times N_T$ weight matrix given by
\begin{equation}
	\bW = \frac{1}{\sigma^2(1-\eta)}\left(  \bI_{N_T} - \bU_1 \left(\frac{1-\eta}{\eta}\bSigma_1^{-1} + \bI_{k}\right) ^{-1}\bU_1^T \right)   \label{eq:weight}
\end{equation}

%Consider the linear regression problem where we have a continuous response $\by\in\mathbb{R}^{n}$
%and let $\bX$ be the design matrix with $n$ rows and $p$ columns where $n$ is the sample size of the raw data. If an intercept is used in the model, we let the first column of $\bX$ be a vector of 1.
Assume that we the predictors in the design matrix $\bX \in \mathbb{R}^{N_T \times p}$ belong to $K$ groups and that the group membership is already defined such that $(1,2,\ldots,p)=\bigcup_{k=1}^{K}I_{k}$ and the cardinality of index set $I_{k}$ is $p_{k}$, $I_{k}\bigcap I_{k^{\prime}}=\emptyset$ for $k\neq k^{\prime},1\le k,k^{\prime}\le K$. Thus group $k$ contains $p_{k}$ predictors, which are $x_{j}$'s for $j\in I_{k}$, and $1\le k\le K.$ If an intercept is included, then $I_{1}=\{1\}$. Given the group partition, we use $\bbeta_{(k)}$ to denote the segment of $\bbeta$ corresponding to group $k$. This notation is used for any $p$-dimensional vector.
%In a more compact form, and introducing observation weights
%\[
%L(\bbeta\mid\bD)=\frac{1}{2}\left[Y-\hat{Y}\right]^{\top}\mathbf{W}\left[Y-\hat{Y}\right]
%\]
%where $\hat{Y}=\sum_{j=1}^{p}\beta_{j}X_{j}$, $\bD$ is the working data $\lbrace \by, \bX \rbrace$, $\bW$ is an $N_T \times N_T$ weight matrix. Then the problem we consider can be expressed as
We consider the group lasso penalized estimator
\begin{equation}
	\min_{\bbeta}L(\bbeta \mid \bD)+\lambda\sum_{k=1}^{K}w_{k}\|\bbk\|_{2},\label{eq:wlslasso}
\end{equation}

The loss function $L$ satisfies the quadratic majorization (QM) condition, since there exists
a $p\times p$ matrix $\bH=\bX^{\trans}\mathbf{W}\bX$, and $\nabla L(\bbeta|\bD)=-\left(Y-\hat{Y}\right)^{\top}\mathbf{W}\bX$, which may only depend on the data $\bD$, such that for all $\bbeta,\bbeta^{*}$,
\begin{equation}
	L(\bbeta\mid\bD)\le L(\bbeta^{*}\mid\bD)+(\bbeta-\bbeta^{*})^{\trans}\nabla L(\bbeta^{*}|\bD)+\frac{1}{2}(\bbeta-\bbeta^{*})^{\trans}\bH(\bbeta-\bbeta^{*}).\label{QM1}
\end{equation}

\subsection{Algorithm}

Noticing that the penalty term $\sum_{k=1}^{K}w_{k}||\bbeta_{(k)}||_{2}$ is separable with respect to the indices of the features $k=1, \ldots, K$, we can derive the \textit{groupwise-majorization-descent} (GMD) algorithm for computing the solution of~\eqref{eq:wlslasso} when the loss function satisfies the QM condition. Let $\widetilde{\bbeta}$ denote the current solution of $\bbeta$. Without loss of generality, let us derive the GMD update of $\bbkt$, the coefficients of group $k$. Define $\bH_{k}$ as the sub-matrix of $\bH$ corresponding to group $k$. For example, if group 2 is $\{2,4\}$ then $\bH_{(2)}$ is a $2\times2$ matrix with
\[
\bH_{(2)}=\left[\begin{array}{cc}
h_{2,2} & h_{2,4}\\
h_{4,2} & h_{4,4}
\end{array}\right],
\]

where $h_{i,j}$ is the $i,j$th entry of the $\bH$ matrix. Write $\bbeta$ such that $\bbeta_{(k^{\prime})}=\widetilde{\bbeta}_{(k^{\prime})}$ for $k^{\prime}\ne k$. Given $\bbeta_{(k^{\prime})}=\widetilde{\bbeta}_{(k^{\prime})}$ for $k^{\prime}\ne k$, the optimal $\bbk$ is defined as
\begin{equation}
	\arg\min_{\boldsymbol{\beta}^{(k)}}L(\bbeta\mid\bD)+\lambda w_{k}\Vert\bbk\Vert_{2}.\label{GMDeq1}
\end{equation}
Unfortunately, there is no closed form solution to~\eqref{GMDeq1} for a general loss function with general design matrix. We overcome the computational obstacle by taking advantage of the QM condition. From~\eqref{QM1} we have
\[
L(\bbeta\mid\bD)\le L(\widetilde{\bbeta}\mid\bD)+(\bbeta-\widetilde{\bbeta})^{\trans}\nabla L(\widetilde{\bbeta}|\bD)+\frac{1}{2}(\bbeta-\widetilde{\bbeta})^{\trans}\bH(\bbeta-\widetilde{\bbeta}).
\]

Write $U(\widetilde{\bbeta})=-\nabla L(\widetilde{\bbeta}|\bD)$. Using
\[
\bbeta-\widetilde{\bbeta}=(\underbrace{0,\ldots,0}_{k-1},\bbk-\bbkt,\underbrace{0,\ldots,0}_{K-k}),
\]
we can write
\begin{equation}
	L(\bbeta\mid\bD)\le L(\widetilde{\bbeta}\mid\bD)-(\bbk-\bbkt)^{\trans}U_{(k)}+\frac{1}{2}(\bbk-\bbkt)^{\trans}\bH_{(k)}(\bbk-\bbkt).\label{GMDeq2}
\end{equation}
where
\begin{align}
	U_{(k)} & =\frac{\partial}{\partial\bbk}L_{Q}(\bbeta\mid\bD)=-\left(Y-\hat{Y}\right)^{\top}\mathbf{W}\mathbf{X}_{(k)},\label{eq:gradientj-1}\\
	\mathbf{H}_{(k)} & =\frac{\partial^{2}}{\partial\bbk\partial\bbk^{\top}}L_{Q}(\bbeta\mid\bD)=\mathbf{X}_{(k)}^{\top}\mathbf{W}\mathbf{X}_{(k)}.\label{eq:hessianj-1}
\end{align}

Let $\eta_{k}$ be the largest eigenvalue of $\bH_{(k)}$. We set $\gamma_{k}=(1+\varepsilon^{*})\eta_{k}$, where $\varepsilon^{*}=10^{-6}$. Then we can further relax the upper bound in~\eqref{GMDeq2} as
\begin{equation}
	L(\bbeta\mid\bD)\leq L(\widetilde{\bbeta}\mid\bD)-(\bbeta^{(k)}-\widetilde{\bbeta}^{(k)})^{\trans}U_{(k)}+\frac{1}{2}\gamma_{k}(\bbeta^{(k)}-\widetilde{\bbeta}^{(k)})^{\trans}(\bbeta^{(k)}-\widetilde{\bbeta}^{(k)}).\label{GMDeq3-1}
\end{equation}
It is important to note that the inequality strictly holds unless for $\bbeta^{(k)}=\widetilde{\bbeta}^{(k)}$. Instead of minimizing~\eqref{GMDeq1} we solve
\begin{equation}
	\arg\min_{\bbeta^{(k)}}L(\widetilde{\bbeta}\mid\bD)-(\bbeta^{(k)}-\widetilde{\bbeta}^{(k)})^{\trans}U_{(k)}+\frac{1}{2}\gamma_{k}(\bbeta^{(k)}-\widetilde{\bbeta}^{(k)})^{\trans}(\bbeta^{(k)}-\widetilde{\bbeta}^{(k)})+\lambda w_{k}\Vert\bbeta^{(k)}\Vert_{2}.\label{GMDeq4-1}
\end{equation}

Denote by $\widetilde{\bbeta}^{(k)}(\textrm{new})$ the solution to~\eqref{GMDeq4-1}. It is straightforward to see that $\widetilde{\bbeta}^{(k)}(\textrm{new})$ has a simple closed-from expression
\begin{equation}
	\widetilde{\bbeta}^{(k)}(\textrm{new})=\frac{1}{\gamma_{k}}\left(U^{(k)}+\gamma_{k}\widetilde{\bbeta}^{(k)}\right)\left(1-\frac{\lambda w_{k}}{\Vert U^{(k)}+\gamma_{k}\widetilde{\bbeta}^{(k)}\Vert_{2}}\right)_{+}.\label{GMDeq5-1}
\end{equation}

Algorithm~\ref{alg1} summarizes the details of GMD.

\begin{algorithm}
	\begin{enumerate}
		\item For $k=1,\ldots,K$, compute $\gamma_k$, the largest eigenvalue of $\bH^{(k)}$.
		\item Initialize $\widetilde \bbeta$.
		\item Repeat the following cyclic groupwise updates until convergence:
		\begin{enumerate}
			\item[---] for $k=1,\ldots,K$, do step (3.1)--(3.3)
			\begin{enumerate}
				\item[3.1]
				Compute $U(\widetilde \bbeta )=-\nabla L(\widetilde \bbeta | \bD)$.
				\item[3.2]
				Compute
				$
				\widetilde \bbeta^{(k)}(\textrm{new}) = \frac{1}{\gamma_k}\left( U^{(k)}+\gamma_k \widetilde \bbeta^{(k)} \right)\left(1-\frac{\lambda w_k}{\Vert U^{(k)}+\gamma_k \widetilde \bbeta^{(k)} \Vert_2}\right)_{+} .
				$
				\item[3.3]
				Set $\widetilde \bbeta^{(k)}=\widetilde \bbeta^{(k)}(\textrm{new})$.
			\end{enumerate}
		\end{enumerate}
	\end{enumerate}
	\caption{The GMD algorithm for general group-lasso learning. \label{alg1}}
\end{algorithm}


\subsection{Convergence}

We can prove the strict descent property of GMD by using the MM principle \citep{MM1,hunter2004tutorial,MM08}. Define
\begin{equation}
	Q(\bbeta \mid \bD)=L(\widetilde \bbeta \mid \bD)-(\bbeta^{(k)}-\widetilde \bbeta^{(k)})^{\trans}
	U^{(k)}+\frac{1}{2} \gamma_k (\bbeta^{(k)}-\widetilde \bbeta^{(k)})^{\trans} ( \bbeta^{(k)}- \widetilde \bbeta^{(k)})+\lambda w_k \Vert \bbeta^{(k)}\Vert_2.
\end{equation}
Obviously, $Q(\bbeta \mid \bD)=L(\bbeta \mid \bD)+\lambda w_k \Vert \bbeta^{(k)}\Vert_2$ when $\bbeta^{(k)}=\widetilde \bbeta^{(k)}$ and
(\ref{GMDeq3}) shows that
$Q(\bbeta \mid \bD) > L(\bbeta \mid \bD)+\lambda w_k \Vert \bbeta^{(k)}\Vert_2$ when $\bbeta^{(k)} \neq \widetilde \bbeta^{(k)}$.
After updating $\widetilde \bbeta^{(k)}$ using (\ref{GMDeq5}), we have
\begin{eqnarray*}
	L(\widetilde \bbeta^{(k)}(\textrm{new}) \mid \bD)+\lambda w_k \Vert \widetilde \bbeta^{(k)}(\textrm{new}) \Vert_2
	&  \le &  Q(\widetilde \bbeta^{(k)}(\textrm{new})  \mid \bD)\\
	& \le & Q(\widetilde \bbeta  \mid \bD) \\
	& = & L(\widetilde \bbeta \mid \bD)+\lambda w_k \Vert \widetilde \bbeta^{(k)}\Vert_2.
\end{eqnarray*}
Moreover, if $\widetilde \bbeta^{(k)}(\textrm{new}) \neq \widetilde \bbeta^{(k)}$, then the first inequality becomes
\begin{eqnarray*}
	L(\widetilde \bbeta^{(k)}(\textrm{new}) \mid \bD)+\lambda w_k \Vert \widetilde \bbeta^{(k)}(\textrm{new}) \Vert_2
	&  < &  Q(\widetilde \bbeta^{(k)}(\textrm{new})  \mid \bD).
\end{eqnarray*}
Therefore, the objective function is strictly decreased after updating all groups in a cycle, unless the solution does not change after each groupwise update. If this is the case,
we can show that the solution must satisfy the KKT conditions, which means that the algorithm converges and finds the right answer. To see this,
if $\widetilde \bbeta^{(k)}(\textrm{new}) = \widetilde \bbeta^{(k)}$ for all $k$, then by the update formula \eqref{GMDeq5-1} we have that for all $k$
\begin{align}\label{KKTcond1}
	\widetilde \bbeta^{(k)} = \frac{1}{\gamma_k}\left( U^{(k)}+\gamma_k \widetilde \bbeta^{(k)} \right)\left(1-\frac{\lambda w_k}{\Vert U^{(k)}+\gamma_k \widetilde \bbeta^{(k)} \Vert_2}\right) \qquad\textrm{if }\Vert U^{(k)}+\gamma_k \widetilde \bbeta^{(k)} \Vert_2 > \lambda w_{k},\\\label{KKTcond2}
	\widetilde \bbeta^{(k)} = \boldsymbol{0} \qquad\textrm{if }\Vert U^{(k)}+\gamma_k \widetilde \bbeta^{(k)} \Vert_2 \leq \lambda w_{k}.
\end{align}
By straightforward algebra  we obtain the KKT conditions:
\begin{align*}
	-U^{(k)}+\lambda w_{k}\cdot\frac {\widetilde\bbeta^{(k)} }{\Vert\widetilde\bbeta^{(k)}\Vert_2}=\boldsymbol{0}\qquad\textrm{if }\widetilde\bbeta^{(k)}\neq \boldsymbol{0},\\
	\left\Vert
	U^{(k)}
	\right\Vert_2 \le\lambda w_{k}\qquad\textrm{if }\widetilde\bbeta^{(k)}=\boldsymbol{0},
\end{align*}
where $k=1,2,\ldots,K$. Therefore, if the objective function stays unchanged after a cycle, the algorithm necessarily converges to the right
answer.




\subsection{Fitting Options and Algorithms}
Recall $\mb{K} \in \mathbb{R}^{N_T\times k}$ is the matrix containing the $k$ SNPs used to compute the factored kinship matrix $\bPhi$. The dimension of this matrix will determine the algorithm used as shown in the table below.
\ctable[caption={Algorithm used based on dimension of $\mb{K}$. },label=tab:review,pos=h!,doinside=\footnotesize]{LLLLL}{
}{
\FL
Dimension of $\mb{K}$    & lasso   & group lasso \ML
$N_T > k$  			& gcdnet (or degenerate gglasso)  & gglasso (GMD Algorithm with weight matrix) \\
$N_T < k$              & glmnet (Coordinate descent with observation weights)   & gglasso (GMD Algorithm with observation weights)
\LL
}
